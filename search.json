[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Clojure Tidy Tuesdays",
    "section": "",
    "text": "Welcome to Clojure Tidy Tuesdays!\nThis is a collection of #TidyTuesday explorations in Clojure. Tidy Tuesdays are an initiative of the R for data science online learning community, and as a Clojure enthusiast, I’m publishing implementations of the data-generation scripts and data explorations in Clojure here in 2024. Follow along to learn all about Clojure’s rich data ecosystem!\nYou can find more code showing examples and explanations about how to download and wrangle data in the data fetching scripts in the data folder of this project’s repo. They’re not published here as namespaces because many make lots of API calls and/or don’t display nicely as published notebooks.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome to Clojure Tidy Tuesdays!</span>"
    ]
  },
  {
    "objectID": "index.html#goals",
    "href": "index.html#goals",
    "title": "Clojure Tidy Tuesdays",
    "section": "Goals",
    "text": "Goals\nSome goals for this project this year are:\n\nTo increase the number of non-trivial examples and guides on how to work with Clojure’s emerging data science stack\nTo work out bugs and rough edges in these tools and libraries to help develop them for more professional use\nTo learn how to use them better myself, as part of writing the Clojure Data Cookbook",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome to Clojure Tidy Tuesdays!</span>"
    ]
  },
  {
    "objectID": "index.html#support",
    "href": "index.html#support",
    "title": "Clojure Tidy Tuesdays",
    "section": "Support",
    "text": "Support\nIf you find any of this useful, the best help is to share it with someone else you think might learn something from it.\nYou can also follow me for updates on this and other projects on Mastodon, LinkedIn, or on the various chat rooms and forums where Clojure people hang out (like the Clojurians Zulip instance or Clojurians Slack).\nLastly, you can also support this work financially. This work is made possible by the ongoing funding I receive from Clojurists together and my generous Github Sponsors. If you find this work valuable, please consider contributing financially to it’s sustainability:\n\n\n\n\n\n\nsource: src/index.clj",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome to Clojure Tidy Tuesdays!</span>"
    ]
  },
  {
    "objectID": "year_2024.week_1.analysis.html",
    "href": "year_2024.week_1.analysis.html",
    "title": "Week 1 - Holiday Movies",
    "section": "",
    "text": "Making a bar chart\nTo get started, we’ll load the data into our notebook, starting with the dataset of holiday movies:\nWe’ll make a graph that’s similar to the one in this article about Christmas movies, showing the top 20 movies by number of votes. For this we’ll use noj, a library that nicely integrates hanami, a Clojure library that wraps vega-lite, with tablecloth. First we can tell it to make a bar chart:\nWe have to tell hanami what values to use in the chart and what they are:\nThis works! We’ll give it a few more options to tidy up the chart, like sorting the bars by number of votes rather than alphabetically and re-labelling the axes:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1 - Holiday Movies</span>"
    ]
  },
  {
    "objectID": "year_2024.week_1.analysis.html#making-a-bar-chart",
    "href": "year_2024.week_1.analysis.html#making-a-bar-chart",
    "title": "Week 1 - Holiday Movies",
    "section": "",
    "text": "(def holiday-movies\n  (tc/dataset \"data/year_2024/week_1/holiday-movies.csv\"))\n\n\n\n(-&gt; holiday-movies\n    ;; sort the movies by number of votes\n    (tc/order-by \"num_votes\" :desc)\n    ;; select the first 20\n    (tc/select-rows (range 20))\n    ;; make a bar chart\n    (hanami/plot ht/bar-chart {}))\n\n\n\n\n\n\n(-&gt; holiday-movies\n    (tc/order-by \"num_votes\" :desc)\n    (tc/select-rows (range 20))\n    (hanami/plot ht/bar-chart {:X \"num_votes\"\n                               :Y \"primary_title\"\n                               :YTYPE \"nominal\"}))\n\n\n\n\n\n\n(-&gt; holiday-movies\n    (tc/order-by \"num_votes\" :desc)\n    (tc/select-rows (range 20))\n    (hanami/plot ht/bar-chart {:X \"num_votes\"\n                               :Y \"primary_title\"\n                               :YTYPE \"nominal\"\n                               :YTITLE \"Title\"\n                               :XTITLE \"Number of votes\"\n                               :YSORT \"-x\"}))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1 - Holiday Movies</span>"
    ]
  },
  {
    "objectID": "year_2024.week_1.analysis.html#adding-labels-to-the-bar-chart",
    "href": "year_2024.week_1.analysis.html#adding-labels-to-the-bar-chart",
    "title": "Week 1 - Holiday Movies",
    "section": "Adding labels to the bar chart",
    "text": "Adding labels to the bar chart\nIn order to add the number of votes values next to the bars we’ll have to rearrange some of the vega lite spec.. making this tidier and more intuitive is on the list of projects the Clojure data community is actively working on. I get that right now it requires too much information of vega-lite internals. Also it breaks the sorting and I don’t know why yet..\n\n(-&gt; holiday-movies\n    (tc/order-by \"num_votes\" :desc)\n    (tc/select-rows (range 20))\n    (hanami/plot (assoc ht/layer-chart :encoding :ENCODING)\n                 {:TITLE \"Christmas Movies\"\n                  :X \"num_votes\"\n                  :Y \"primary_title\"\n                  :YTYPE \"nominal\"\n                  :YTITLE \"Title\"\n                  :XTITLE \"Number of votes\"\n                  :YSORT \"-x\"\n                  :WIDTH 500\n                  :XSTACK nil\n                  :LAYER [{:mark {:type \"bar\"}}\n                          {:mark {:type \"text\" :align \"left\" :baseline \"middle\" :dx 3}\n                           :encoding {:text {:field \"num_votes\"}}}]}))\n\n\n\n\nThis is a similar idea to what’s accomplished in the article. We don’t have exactly the same data, so we can’t encode the Oscar status of each movie in the colour, but we could do something else, like indicate the decade in which the film was released. There are lots of ways to accomplish this. There would be a way to tell vega-lite to do it (via Hanami), but for the sake of exploring Clojure some more we’ll do the data wrangling part with it. To do that we’ll start with our dataset of the 20 most popular movies and add a column for the release decade, then update our plot to have the bars coloured by decade.\n\n(-&gt; holiday-movies\n    (tc/order-by \"num_votes\" :desc)\n    (tc/select-rows (range 20))\n    ;; add a new column for the decade, computed from the year\n    (tc/map-columns \"decade\" [\"year\"] (fn [year]\n                                        (-&gt; year\n                                            (/ 10)\n                                            (Math/floor)\n                                            (* 10)\n                                            int\n                                            (str \"s\"))))\n    ;; colour our bars according to the decade\n    (hanami/plot ht/bar-chart {:X \"num_votes\"\n                               :Y \"primary_title\"\n                               :YTYPE \"nominal\"\n                               :YTITLE \"Title\"\n                               :XTITLE \"Number of votes\"\n                               :YSORT \"-x\"\n                               :COLOR \"decade\"}))\n\n\n\n\nWithout the bar labels, this is reasonably straightforward.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1 - Holiday Movies</span>"
    ]
  },
  {
    "objectID": "year_2024.week_1.analysis.html#making-a-word-cloud",
    "href": "year_2024.week_1.analysis.html#making-a-word-cloud",
    "title": "Week 1 - Holiday Movies",
    "section": "Making a word cloud",
    "text": "Making a word cloud\nThe next graph in the article is a graph of relationships between the movie keywords. We don’t have the keywords in our dataset, but we could do something with the words of the movie titles, like a word cloud, where the size of the word reflects its occurrence in the movie titles. First we’ll collect all of the words in all the titles into a list, then use those as the data for the word cloud. This is a little more involved with low-level vega details than is ideal right now, but it’s possible.\n\n(let [data (-&gt; holiday-movies\n               (tc/select-columns \"primary_title\")\n               tc/rows\n               flatten)]\n  (kind/vega {:data\n              [{:name \"table\"\n                :values data\n                :transform\n                [{:type \"countpattern\"\n                  :field \"data\"\n                  :case \"upper\"\n                  :pattern \"[\\\\w']{3,}\"\n                  :stopwords \"(the|a|i'm|like|too|into|ing|for|where|she|he|hers|his|how|who|what|your|yours|it|it's|is|are|we|'til|our|and|but|i'll|this|that|from|with)\"\n                  }\n                 {:type \"formula\",\n                  :as \"angle\",\n                  :expr \"[-45, -25, 0, 25, 45][~~(random() * 5)]\"}\n                 {:type \"formula\",\n                  :as \"weight\",\n                  :expr \"if(datum.text=='CHRISTMAS' |\n                            datum.text=='HOLIDAY' |\n                            datum.text=='HANNUKAH', 600, 300)\"}]}],\n              :scales\n              [{:name \"color\"\n                :type \"ordinal\",\n                :domain {:data \"table\", :field \"text\"},\n                :range [\"red\" \"green\" \"grey\" \"darkred\" \"darkgreen\"]}],\n              :marks\n              [{:type \"text\",\n                :from {:data \"table\"},\n                :encode\n                {:enter\n                 {:text {:field \"text\"},\n                  :align {:value \"center\"},\n                  :baseline {:value \"alphabetic\"},\n                  :fill {:scale \"color\", :field \"text\"}},\n                 :update {:fillOpacity {:value 1}},\n                 :hover {:fillOpacity {:value 0.5}}},\n                :transform\n                [{:fontSizeRange [10 56],\n                  :type \"wordcloud\",\n                  :font \"Helvetica Neue, Arial\",\n                  :size [800 400],\n                  :padding 2,\n                  :fontWeight {:field \"datum.weight\"},\n                  :rotate {:field \"datum.angle\"},\n                  :fontSize {:field \"datum.count\"},\n                  :text {:field \"text\"}}]}]}))\n\n\n\n\nSee you next week :)\n\n\n\n\nsource: src/year_2024/week_1/analysis.clj",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1 - Holiday Movies</span>"
    ]
  },
  {
    "objectID": "year_2024.week_2.analysis.html",
    "href": "year_2024.week_2.analysis.html",
    "title": "Week 2 - Canadian NHL Hockey Player Birth Months",
    "section": "",
    "text": "Canadian Birth Months\nThe data for this analysis are gathered from the NHL API and StatsCan. There are a bunch of files saved in data/year_2024/week_2 you can just use, or if you want to explore how the data is collected and cleaned up you can check out data/year_2024/week_2/generate_dataset.clj. We start with data on all Canadian births from 1991-2022 to see the distribution of Canadian births by month.\n_unnamed [12 3]:\nWe can plot this to make it easier to interpret:\nSo now we can see the distribution of births in Canada by month between 1991 and 2022. It actually looks like Canadians are less likely to be born in the earlier months of the year, with the most births in May, July, August, and September. We can compare this to the expected distribution (if each day had an exactly equal chance of being someone’s birthday) by first constructing a dataset of the “expected” monthly distributions:\nAnd then computing the differences between the “expected” monthly births and the actual Canadian data:\ninner-join [12 5]:\nWe can check the distribution for hockey players next.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2 - Canadian NHL Hockey Player Birth Months</span>"
    ]
  },
  {
    "objectID": "year_2024.week_2.analysis.html#canadian-birth-months",
    "href": "year_2024.week_2.analysis.html#canadian-birth-months",
    "title": "Week 2 - Canadian NHL Hockey Player Birth Months",
    "section": "",
    "text": "(def canadian-births-by-month\n  (-&gt; \"data/year_2024/week_2/canada_births_1991_2022.csv\"\n      (tc/dataset {:key-fn keyword})\n      ;; group them by month\n      (tc/group-by [:month])\n      ;; count all births per month over the time period\n      (tc/aggregate {:country-births #(reduce + (:births %))})\n      ;; make a new column for the percentage of total births that each monthly total represents\n      (as-&gt; country-births-ds\n            (let [total-births (-&gt;&gt; country-births-ds\n                                    :country-births\n                                    (reduce +))]\n              (tc/map-columns country-births-ds\n                              :country-pct\n                              :country-births\n                              (fn [country-births]\n                                (-&gt; country-births\n                                    (/ total-births)\n                                    double)))))))\n\n\ncanadian-births-by-month\n\n\n\n\n\n:month\n:country-births\n:country-pct\n\n\n\n\n1\n941370\n0.08015265\n\n\n2\n886150\n0.07545096\n\n\n3\n995978\n0.08480224\n\n\n4\n983363\n0.08372814\n\n\n5\n1029538\n0.08765969\n\n\n6\n1001064\n0.08523528\n\n\n7\n1042392\n0.08875414\n\n\n8\n1024021\n0.08718995\n\n\n9\n1019490\n0.08680416\n\n\n10\n982729\n0.08367415\n\n\n11\n918434\n0.07819978\n\n\n12\n920185\n0.07834886\n\n\n\n\n\n(-&gt; canadian-births-by-month\n    (hanami/plot (assoc ht/layer-chart :encoding :ENCODING)\n                 {:TITLE \"Canadian births by month 1991-2022\"\n                  :X :country-births\n                  :XTITLE \"Number of births (cumulatively)\"\n                  ;; vega-lite uses JS sorting, which does not sort numbers\n                  ;; properly, so we'll tell it what order to use explicitly\n                  :YSORT [\"1\" \"2\" \"3\" \"4\" \"5\" \"6\" \"7\" \"8\" \"9\" \"10\" \"11\" \"12\"]\n                  :Y :month\n                  :YTITLE \"Month\"\n                  :YTYPE \"nominal\"\n                  :LAYER [{:mark \"bar\"}\n                          {:mark {:type \"text\" :align \"left\" :dx 3}\n                           :encoding {:text {:field :country-pct :format \".1%\"}}}]}))\n\n\n\n\n\n\n(def expected-births-by-month\n  (let [months [1 2 3 4 5 6 7 8 9 10 11 12]]\n    (tc/dataset {:month months\n                 :expected-births (map (fn [month]\n                                         (double (cond\n                                                   (#{4 6 9 11} month) 30/365\n                                                   (= month 2) 28/365\n                                                   :else 31/365)))\n                                       months)})))\n\n\n\n(-&gt; expected-births-by-month\n    (tc/inner-join canadian-births-by-month :month)\n    (tc/map-columns :difference [:expected-births :country-pct]\n                    (fn [expected country]\n                      (- expected country))))\n\n\n\n\n\n\n\n\n\n\n\n\n:month\n:expected-births\n:country-births\n:country-pct\n:difference\n\n\n\n\n1\n0.08493151\n941370\n0.08015265\n0.00477885\n\n\n2\n0.07671233\n886150\n0.07545096\n0.00126136\n\n\n3\n0.08493151\n995978\n0.08480224\n0.00012927\n\n\n4\n0.08219178\n983363\n0.08372814\n-0.00153635\n\n\n5\n0.08493151\n1029538\n0.08765969\n-0.00272818\n\n\n6\n0.08219178\n1001064\n0.08523528\n-0.00304350\n\n\n7\n0.08493151\n1042392\n0.08875414\n-0.00382263\n\n\n8\n0.08493151\n1024021\n0.08718995\n-0.00225844\n\n\n9\n0.08219178\n1019490\n0.08680416\n-0.00461238\n\n\n10\n0.08493151\n982729\n0.08367415\n0.00125735\n\n\n11\n0.08219178\n918434\n0.07819978\n0.00399201\n\n\n12\n0.08493151\n920185\n0.07834886\n0.00658264",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2 - Canadian NHL Hockey Player Birth Months</span>"
    ]
  },
  {
    "objectID": "year_2024.week_2.analysis.html#canadian-nhl-hockey-player-birth-months",
    "href": "year_2024.week_2.analysis.html#canadian-nhl-hockey-player-birth-months",
    "title": "Week 2 - Canadian NHL Hockey Player Birth Months",
    "section": "Canadian NHL Hockey Player Birth Months",
    "text": "Canadian NHL Hockey Player Birth Months\nWe have a dataset that includes player info for all NHL players over all time. We’ll start by doing a similar breakdown to the one we did above for all Canadians.\n\n(def nhl-player-births-by-month\n  ;; first, load the dataset:\n  (-&gt; \"data/year_2024/week_2/nhl-player-births.csv\"\n      (tc/dataset {:key-fn keyword})\n      ;; filter out just Canadian players\n      (tc/select-rows #(= (:birth-country %) \"CAN\"))\n      ;; group the players by birth month\n      (tc/group-by [:birth-month])\n      ;; count the number of players per month\n      (tc/aggregate {:births tc/row-count})\n      ;; we'll add a new column where we compute the percentage of births that each monthly total represents\n      (as-&gt; player-births-ds\n          (let [total-births (-&gt;&gt; player-births-ds\n                                  :births\n                                  (reduce +))]\n            (tc/map-columns player-births-ds\n                            :player-pct\n                            :births\n                            (fn [births]\n                              (-&gt; births\n                                  (/ total-births)\n                                  double)))))))\n\nWe can make a similar bar chart to see how the distribution of hockey player births compares to the all-Canada data:\n\n(hanami/plot nhl-player-births-by-month\n             (assoc ht/layer-chart :encoding :ENCODING)\n             {:TITLE \"NHL player births by month\"\n              :X :births\n              :XTITLE \"Number of births\"\n              ;; vega-lite uses JS sorting, which does not sort numbers properly, so we'll tell it what order to use explicitly\n              :YSORT [\"JANUARY\"\n                      \"FEBRUARY\"\n                      \"MARCH\"\n                      \"APRIL\"\n                      \"MAY\"\n                      \"JUNE\"\n                      \"JULY\"\n                      \"AUGUST\"\n                      \"SEPTEMBER\"\n                      \"OCTOBER\"\n                      \"NOVEMBER\"\n                      \"DECEMBER\"]\n              :Y :birth-month\n              :YTITLE \"Month\"\n              :YTYPE \"nominal\"\n              :LAYER [{:mark \"bar\"}\n                      {:mark {:type \"text\" :align \"left\" :dx 3}\n                       :encoding {:text {:field :player-pct\n                                         :format \".1%\"}}}]})\n\n\n\n\nThese are definitely more distributed disproportionately through the early months of the year. We can compare them on top of each other to see more clearly next.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2 - Canadian NHL Hockey Player Birth Months</span>"
    ]
  },
  {
    "objectID": "year_2024.week_2.analysis.html#comparing-all-canadian-births-to-canadian-nhl-player-births",
    "href": "year_2024.week_2.analysis.html#comparing-all-canadian-births-to-canadian-nhl-player-births",
    "title": "Week 2 - Canadian NHL Hockey Player Birth Months",
    "section": "Comparing all Canadian births to Canadian NHL player births",
    "text": "Comparing all Canadian births to Canadian NHL player births\nWe’ll simplify and combine the datasets first, before visualizing them:\n\n(def combined-births-by-month\n  (let [canada-births (-&gt; canadian-births-by-month\n                        ;; clean up the month names so they are the same in both datasets\n                          (tc/update-columns {:month (fn [col]\n                                                       (map #(-&gt; % Month/of .name str/capitalize)\n                                                            col))}))\n        player-births (-&gt; nhl-player-births-by-month\n                          (tc/rename-columns {:birth-month :month\n                                              :births :player-births})\n                          (tc/update-columns {:month (partial map str/capitalize)}))\n        expected-births (-&gt; expected-births-by-month\n                            (tc/rename-columns {:expected-births :expected-births-pct})\n                            (tc/update-columns {:month (fn [col]\n                                                         (map #(-&gt; % Month/of .name str/capitalize)\n                                                              col))}))]\n    ;; join the datasets by month to make one dataset:\n    (-&gt; canada-births\n        (tc/inner-join player-births :month)\n        (tc/inner-join expected-births :month))))\n\nThe visualization in the linked article is a really great example of where ggplot really shines. I think it would be cool to explore how this might be possible to re-create with vega-lite (via the tools and wrappers we have around it Clojure), but for now we can visualize the data in a more simple way using a grouped bar chart. Since this is a pretty custom chart, I’m going to hand-write the vega-lite spec. There’s tons to learn about vega-lite, but the main takeaway for now is that you can pass any vega-lite (or vega) spec to Clojure’s vega-viz wrappers and it will render them:\n\n(hanami/plot combined-births-by-month\n             {:data {:values :DATA\n                     :format :DFMT}\n              :repeat {:layer [:expected-births-pct :country-pct :player-pct]}\n              :spec {:encoding {:y {:field :month\n                                    :type \"nominal\"\n                                    :axis {:title \"Month\"}\n                                    :sort [\"January\"\n                                           \"February\"\n                                           \"March\"\n                                           \"April\"\n                                           \"May\"\n                                           \"June\"\n                                           \"July\"\n                                           \"August\"\n                                           \"September\"\n                                           \"October\"\n                                           \"November\"\n                                           \"December\"]}}\n                     :layer [{:mark \"bar\"\n                              :height {:step \"12\"}\n                              :encoding {:x {:field {:repeat \"layer\"}\n                                             :type \"quantitative\"\n                                             :axis {:title \"Percentage of births\"\n                                                    :format \".0%\"}}\n                                         :color {:datum {:repeat \"layer\"}\n                                                 :type \"nominal\"\n                                                 :scale {:range [\"steelblue\" \"red\" \"black\"]}\n                                                 :legend {:title \"Group\"\n                                                          :labelExpr \"{'country-pct': 'Canada', 'player-pct': 'NHL Players', 'expected-births-pct': 'Expected'}[datum.label]\"}}\n                                         :yOffset {:datum {:repeat \"layer\"}}}}\n                             {:mark {:type \"text\" :dx -80 :fontSize 10.5}\n                              :encoding {:text {:field {:repeat \"layer\"} :format \".1%\"}\n                                         :color {:value \"white\"}\n                                         :yOffset {:datum {:repeat \"layer\"}}}}]}}\n             {})\n\n\n\n\nWhen the data are all side-by-side like this we can see pretty clearly that it’s true that NHL players are in fact more likely to be born in the earlier parts of the year.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2 - Canadian NHL Hockey Player Birth Months</span>"
    ]
  },
  {
    "objectID": "year_2024.week_2.analysis.html#chi-squared-test",
    "href": "year_2024.week_2.analysis.html#chi-squared-test",
    "title": "Week 2 - Canadian NHL Hockey Player Birth Months",
    "section": "Chi-squared test",
    "text": "Chi-squared test\nThe last thing the author of the original article does is a chi-squared test on the NHL player births distribution. A chi-squared test is one you can use to see whether a given variable follows a hypothesized distribution in a more quantitative way, so it’s basically a more stats-y way to show that the NHL player birth dates distribution is different than we would expect. I.e., it can answer, quantitatively, the question “what are the chances that a sample of birth dates would be distributed the way our NHL player birth dates dataset is, compared to the expected distribution?” In this case our “expected” distribution is based on the distribution by month of all Canadian births.\nIn Clojure, this test is already implemented in the fastmath library, but in the interest of teaching (and because I already did it the hard way first before I learned how to use the fastmath function), we can just do it manually.\nFirst we compute the chi statistic. The formula is \\[\\sum_{k=1}^{n}\\frac{(O_k - E_k)^2}{E_k}\\] where O is the actual value and E is the expected one. We can implement this in Clojure. The first thing we need is the expected values. Our “actual” values are the count of NHL player births by month, so we can compute the expected ones by applying the expected distribution (based on the Canadian births data):\n\n(def actual-and-expected-births\n  ;; first get the total count of player births\n  (let [total-player-births (-&gt;&gt; (tc/select-columns nhl-player-births-by-month [:births])\n                                 tc/rows\n                                 (map first)\n                                 (reduce +))\n\n      ;; clean up the month names so they are the same in both datasets\n        canada-births (-&gt; canadian-births-by-month\n                          (tc/update-columns {:month (fn [col]\n                                                       (map #(-&gt; % Month/of .name str/capitalize)\n                                                            col))}))]\n    ;; combine the Canadian data with the NHL player data to calculate the expected values\n    (-&gt; nhl-player-births-by-month\n        (tc/rename-columns {:birth-month :month\n                            :births :actual})\n        (tc/update-columns {:month (partial map str/capitalize)})\n        (tc/inner-join canada-births :month)\n        (tc/map-columns :expected\n                        [:country-pct]\n                        (fn [pct]\n                          (Math/round (* pct total-player-births))))\n        (tc/select-columns [:month :actual :expected]))))\n\n\nactual-and-expected-births\n\ninner-join [12 3]:\n\n\n\n:month\n:actual\n:expected\n\n\n\n\nJanuary\n542\n438\n\n\nFebruary\n533\n412\n\n\nMarch\n513\n464\n\n\nApril\n526\n458\n\n\nMay\n520\n479\n\n\nJune\n463\n466\n\n\nJuly\n437\n485\n\n\nAugust\n394\n477\n\n\nSeptember\n421\n475\n\n\nOctober\n383\n457\n\n\nNovember\n374\n428\n\n\nDecember\n361\n428\n\n\n\nThis gives us the expected values – the number of births we would expect per month if we randomly sampled the same number of births from the Canadian data as we have NHL player births. Now we can compute the chi squared statistic:\n\n(def chi-squared\n  (-&gt;&gt; (tc/select-columns actual-and-expected-births [:actual :expected])\n       tc/rows\n       (map (fn [[actual expected]]\n              ;; find the difference between the actual and expected values\n              (let [a-e (- actual expected)]\n                (-&gt; a-e\n                    ;; square it\n                    (* a-e)\n                    ;; divide it by the expected value\n                    (/ expected)))))\n       ;; sum them\n       (reduce +)\n       ;; make it a number instead of a fraction\n       double\n       Math/round))\n\n\nchi-squared\n\n\n134\n\nIn order to use this value to test whether our NHL player birth dates follow an “expected” distribution, we can calculate the P-value using a chi-squared distribution. This is also implemented in fastmath, in this case we’ll use it. The P value tells us what the probability is of observing the given discrepancy between the actual and expected values. The degrees of freedom is one less than the number of categories we have, so in our case 11.\n\n(require '[fastmath.stats :as stats])\n\n\n(require '[fastmath.random :as r])\n\n\n(stats/p-value (r/distribution :chi-squared {:degrees-of-freedom 11}) chi-squared)\n\n\n0.0\n\nInterpreting this, we can say there is a 0.0% chance that the NHL player birth data are sampled from the Canadian birth data. In other words, the probability that the NHL player birth distribution by month is a fluke is 0. It’s definitely anomalous, not a result of not having a large enough sample size.\nSo that wraps up our exploration of this data! I’d love to poke around some more and see how the distribution of NHL players by nationality breaks down over time, and play around with vega-lite some more to make cooler looking graphs. But here I am already late for next week’s tidy tuesday, so I’ll move on for now and come back once I’ve learned more.\nSee you next week :)\n\n\n\n\nsource: src/year_2024/week_2/analysis.clj",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2 - Canadian NHL Hockey Player Birth Months</span>"
    ]
  },
  {
    "objectID": "year_2024.week_3.analysis.html",
    "href": "year_2024.week_3.analysis.html",
    "title": "Week 3 - US Polling Places 2012-2020",
    "section": "",
    "text": "Polling location counts over time\nIn order to see which states have had the largest shifts in polling location counts over time, we can group the data by election date and state and then count the number of rows in each group:\n_unnamed [161 3]:\nWe can plot this data to get a glimpse of how the count of polling places per state has changed over time:\nTo make this chart easier to interpret, we’ll change a few things, noted inline below:\nEven this doesn’t really tell us much, other than that most states saw no change in the overall number of polling places over the years (lots of straight lines!). To get a better idea of how the states compare to each other, we can facet this graph by state.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3 - US Polling Places 2012-2020</span>"
    ]
  },
  {
    "objectID": "year_2024.week_3.analysis.html#polling-location-counts-over-time",
    "href": "year_2024.week_3.analysis.html#polling-location-counts-over-time",
    "title": "Week 3 - US Polling Places 2012-2020",
    "section": "",
    "text": "(def all-polling-locations\n  (-&gt; \"data/year_2024/week_3/polling-places.csv\"\n      (tc/dataset {:key-fn keyword})))\n\n\n(-&gt; all-polling-locations\n    (tc/group-by [:election_date :state])\n    (tc/aggregate {:count tc/row-count}))\n\n\n\n\n\n:election_date\n:state\n:count\n\n\n\n\n2020-11-03\nAL\n2075\n\n\n2012-11-06\nAK\n396\n\n\n2014-11-04\nAK\n396\n\n\n2016-11-08\nAK\n396\n\n\n2018-11-06\nAK\n396\n\n\n2020-11-03\nAK\n441\n\n\n2012-11-06\nAR\n2536\n\n\n2014-11-04\nAR\n2972\n\n\n2018-11-06\nAR\n8980\n\n\n2020-11-03\nAR\n967\n\n\n…\n…\n…\n\n\n2020-11-03\nVA\n2485\n\n\n2012-11-06\nWV\n1860\n\n\n2014-11-04\nWV\n1816\n\n\n2016-11-08\nWV\n1759\n\n\n2018-11-06\nWV\n1759\n\n\n2020-11-03\nWV\n1729\n\n\n2012-11-06\nWI\n6587\n\n\n2014-11-04\nWI\n6765\n\n\n2016-11-08\nWI\n6865\n\n\n2018-11-06\nWI\n6995\n\n\n2020-11-03\nWI\n7092\n\n\n\n\n\n(-&gt; all-polling-locations\n    (tc/group-by [:election_date :state])\n    (tc/aggregate {:count tc/row-count})\n    (hanami/plot ht/line-chart\n                 {:X :election_date\n                  :XTYPE :temporal\n                  :Y :count\n                  :COLOR {:field \"state\" :type \"nominal\"}}))\n\n\n\n\n\n\n(-&gt; all-polling-locations\n    (tc/group-by [:election_date :state])\n    (tc/aggregate {:count tc/row-count})\n    (hanami/plot (-&gt; ht/line-chart\n                     (assoc-in [:mark :strokeWidth] :MSWIDTH)\n                     (assoc-in [:mark :opacity] :MOPACITY))\n                 {:X :election_date\n                  :XTYPE :temporal\n                  :Y :count\n                  ;; make the whole chart wider\n                  :WIDTH 700\n                  ;; make the line widths slightly bigger and more transparent\n                  :MSWIDTH 4\n                  :MOPACITY 0.5\n                  :COLOR {:field \"state\" :type \"nominal\"\n                          ;; get rid of the legend\n                          :legend nil}\n                  ;; replace the legend with tooltips, to see the state when hovering over a point\n                  :MTOOLTIP true\n                  ;; change the y-axis to a logarithmic scale to make it easier to interpret the\n                  ;; wide range of counts\n                  :YSCALE {:type \"log\"}}))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3 - US Polling Places 2012-2020</span>"
    ]
  },
  {
    "objectID": "year_2024.week_3.analysis.html#faceting-polling-places-over-time-graph",
    "href": "year_2024.week_3.analysis.html#faceting-polling-places-over-time-graph",
    "title": "Week 3 - US Polling Places 2012-2020",
    "section": "Faceting polling places over time graph",
    "text": "Faceting polling places over time graph\n\n(-&gt; all-polling-locations\n    (tc/group-by [:election_date :state])\n    (tc/aggregate {:count tc/row-count})\n    ;; this is pretty messy -- for now we have to manually cobble together\n    ;; the plot specifications in this way, but optimizing these to make\n    ;; them more ergonomic is high on the priority list\n    (hanami/plot (-&gt; ht/line-chart\n                     (assoc :encoding (assoc (hc/get-default :ENCODING)\n                                             :facet\n                                             :FACET)))\n                 {:X :election_date\n                  :XTITLE \"Year\"\n                  :XGRID false\n                  :XTYPE :temporal\n                  :Y :count\n                  :YTITLE \"Count\"\n                  :YGRID false\n                  :HEIGHT 50\n                  :WIDTH 80\n                  :COLOR {:field :state :type :nominal :legend false}\n                  :FACET {:field :state :type :nominal :columns 7 :title \"State\"}}))\n\n\n\n\nThis is still not super informative (IMO), but better. We can at least clearly see the outliers that did see some change over time, i.e. AR, MD, MN. It also reveals states that have no or little data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3 - US Polling Places 2012-2020</span>"
    ]
  },
  {
    "objectID": "year_2024.week_3.analysis.html#relative-changes-in-polling-places-by-state",
    "href": "year_2024.week_3.analysis.html#relative-changes-in-polling-places-by-state",
    "title": "Week 3 - US Polling Places 2012-2020",
    "section": "Relative changes in polling places by state",
    "text": "Relative changes in polling places by state\nNext we can try to visualize the relative changes in polling station counts between two years. We can check out which states have data first and see that it probably doesn’t matter much which two years we choose:\n\n(-&gt; all-polling-locations\n    (tc/select-columns [:election_date :state])\n    (tc/unique-by [:election_date :state])\n    (tc/fold-by [:election_date])\n    (tc/update-columns {:state (partial map count)})\n    (tc/order-by :election_date))\n\n_unnamed [7 2]:\n\n\n\n:election_date\n:state\n\n\n\n\n2012-11-06\n30\n\n\n2014-11-04\n32\n\n\n2016-02-09\n1\n\n\n2016-09-13\n1\n\n\n2016-11-08\n30\n\n\n2018-11-06\n32\n\n\n2020-11-03\n35\n\n\n\nNo year has as many states with data as the most recent year, so for the sake of this comparison we’ll compute the differences in polling station counts between the first and last years. We can do this by “folding” our dataset up by state, rolling up all the rows that share a state into one list:\n\n(-&gt; all-polling-locations\n    ;; first we'll group all the data by election date and state to\n    ;; get our dataset of polling place counts by date and state\n    (tc/group-by [:election_date :state])\n    (tc/aggregate {:count tc/row-count})\n    ;; we'll select only the rows from 2012 and 2020 so we can\n    ;; compare just those two counts\n    (tc/select-rows (fn [row]\n                      (let [year (-&gt; row :election_date .getYear)]\n                        (or (= 2012 year) (= 2020 year)))))\n    ;; sort this dataset by election date\n    (tc/order-by [:election_date])\n    ;; roll up the dates and counts by state\n    (tc/fold-by [:state])\n    ;; select only rows that have data for both elections\n    (tc/select-rows #(= 2 (count (:count %))))\n    ;; now we can compute the difference between the 2012 and 2020\n    ;; counts -- we'll compute this as a relative difference (i.e.\n    ;; relative to the 2012 value), for the sake of comparison, so\n    ;; that we can get a sense of how states compare to each other\n    ;; despite the large discrepancies in absolute numbers of polling places\n    (tc/map-columns :difference [:count] (fn [[c2012 c2020]]\n                                           (/ (- c2020 c2012)\n                                              c2012)))\n    ;; for the sake of seeing the relative differences in counts we only\n    ;; care about the state and our new computed column\n    (tc/select-columns [:state :difference]))\n\n_unnamed [26 2]:\n\n\n\n:state\n:difference\n\n\n\n\nCT\n-0.03364\n\n\nNH\n0.08654\n\n\nME\n0.08621\n\n\nIL\n-0.1534\n\n\nNC\n-0.03411\n\n\nMD\n-0.8340\n\n\nND\n0.5938\n\n\nAK\n0.1136\n\n\nOH\n-0.04132\n\n\nMA\n-0.0009195\n\n\n…\n…\n\n\nDE\n0.004630\n\n\nSC\n0.05645\n\n\nIA\n-0.004147\n\n\nAR\n-0.6187\n\n\nVT\n-0.01509\n\n\nMT\n-0.1501\n\n\nVA\n0.02771\n\n\nWV\n-0.07043\n\n\nNE\n-0.1409\n\n\nWI\n0.07667\n\n\nLA\n-0.07847\n\n\n\nNow we can see quantitatively what the visualizations above seemed to show – most states saw not much change in the number of polling stations. We can visualize this data as a bar chart and see the outliers pretty clearly:\n\n(-&gt; all-polling-locations\n    (tc/group-by [:election_date :state])\n    (tc/aggregate {:count tc/row-count})\n    (tc/select-rows (fn [row]\n                      (let [year (-&gt; row :election_date .getYear)]\n                        (or (= 2012 year) (= 2020 year)))))\n    (tc/order-by [:election_date])\n    (tc/fold-by [:state])\n    (tc/select-rows #(= 2 (count (:count %))))\n    (tc/map-columns :difference [:count] (fn [[c2012 c2020]]\n                                           (-&gt; c2020\n                                               (- c2012)\n                                               (/ c2012)\n                                               double)))\n    (hanami/plot (assoc ht/layer-chart :encoding :ENCODING)\n                 {:X :difference\n                  :XAXIS {:domain false :format \"1%\"\n                          :title \"Change in polling place count 2012-2020\"}\n                  :YSORT \"-x\"\n                  :Y :state\n                  :YTYPE \"nominal\"\n                  :YTITLE \"State\"\n                  :LAYER [{:mark \"bar\"}\n                          {:mark {:type \"text\"\n                                  :align {:expr \"datum.difference &lt; 0 ? 'right' : 'left'\"}\n                                  :dx {:expr \"datum.difference &lt; 0 ? -2 : 2\"}\n                                  }\n                           :encoding {:text {:field :difference\n                                             :format \".1%\"}}}]}))\n\n\n\n\nThere we have it! An interesting investigation of some data on polling places. See you next week :)\n\n\n\n\nsource: src/year_2024/week_3/analysis.clj",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3 - US Polling Places 2012-2020</span>"
    ]
  },
  {
    "objectID": "year_2024.week_4.analysis.html",
    "href": "year_2024.week_4.analysis.html",
    "title": "Week 4 - Educational Attainment of Young People in English Towns",
    "section": "",
    "text": "Plotting the data as a scatterplot\nThe first graph in the article is a kind of scatterplot segmented by the size of the town. We’ll load the data first:\ndata/year_2024/week_4/english_education.csv [1104 31]:\nWe can plot the education scores sorted by town size like in the article to start:\nWe can add jitter to this chart to spread out the points along the y axis since they’re all bunched up because of how we’re sorting them along the y axis:\nThis is slightly better, but it’s already obvious that we’re never going to be able to replicate the graph in the article this way – there’s no way to specify anything precise about how to offset the points along the y axis with the jittering that vega-lite supports. To proceed and try to replicate the graph from the article, we can make a beeswarm plot. For this we’ll have to use vega itself (as opposed to vega-lite). Vega-lite is awesome, but it’s an intentionally less complete (but as a tradeoff simpler) layer on top of vega. Every once in a while I come across a type of graph that is not well supported by vega-lite and drop into vega. Fortunately there are lots of solutions for rendering vega plots in Clojure notebooks (like Clay, which I’m using to build this book).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4 - Educational Attainment of Young People in English Towns</span>"
    ]
  },
  {
    "objectID": "year_2024.week_4.analysis.html#plotting-the-data-as-a-scatterplot",
    "href": "year_2024.week_4.analysis.html#plotting-the-data-as-a-scatterplot",
    "title": "Week 4 - Educational Attainment of Young People in English Towns",
    "section": "",
    "text": "(def english-education\n  (-&gt; \"data/year_2024/week_4/english_education.csv\"\n      (tc/dataset {:key-fn keyword})))\n\n\nenglish-education\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:town11cd\n:town11nm\n:population_2011\n:size_flag\n:rgn11nm\n:coastal\n:coastal_detailed\n:ttwa11cd\n:ttwa11nm\n:ttwa_classification\n:job_density_flag\n:income_flag\n:university_flag\n:level4qual_residents35-64_2011\n:ks4_2012-2013_counts\n:key_stage_2_attainment_school_year_2007_to_2008\n:key_stage_4_attainment_school_year_2012_to_2013\n:level_2_at_age_18\n:level_3_at_age_18\n:activity_at_age_19:_full-time_higher_education\n:activity_at_age_19:_sustained_further_education\n:activity_at_age_19:_appprenticeships\n:activity_at_age_19:employment_with_earnings_above£0\n:activity_at_age_19:employment_with_earnings_above£10,000\n:activity_at_age_19:_out-of-work\n:highest_level_qualification_achieved_by_age_22:_less_than_level_1\n:highest_level_qualification_achieved_by_age_22:_level_1_to_level_2\n:highest_level_qualification_achieved_by_age_22:_level_3_to_level_5\n:highest_level_qualification_achieved_by_age_22:_level_6_or_above\n:highest_level_qualification_achieved_b_age_22:_average_score\n:education_score\n\n\n\n\nE34000007\nCarlton in Lindrick BUA\n5456.0\nSmall Towns\nEast Midlands\nNon-coastal\nSmaller non-coastal town\nE30000291\nWorksop and Retford\nMajority town and city (small)\nResidential\nHigher deprivation towns\nNo university\nLow\n65.0\n65.00000000\n70.76923077\n72.30769231\n50.76923077\n30.76923076923077\n21.53846153846154\n*\n52.307692307692314\n36.92307692307693\n*\n*\n34.9\n39.7\n*\n3.32307692\n-0.53375042\n\n\nE34000016\nDorchester (West Dorset) BUA\n19060.0\nSmall Towns\nSouth West\nNon-coastal\nSmaller non-coastal town\nE30000046\nDorchester and Weymouth\nMajority town and city (small)\nWorking\nMid deprivation towns\nNo university\nMedium\n239.0\n69.05829596\n71.12970711\n85.71428571\n60.08403361\n41.84100418410041\n13.389121338912133\n10.0418410041841\n51.04602510460251\n24.686192468619247\n4.184100418410042\n*\n21.7\n44.6\n33.3\n3.73221757\n1.95201875\n\n\nE34000020\nEly BUA\n19090.0\nSmall Towns\nEast of England\nNon-coastal\nSmaller non-coastal town\nE30000186\nCambridge\nMajority town and city (Large Towns)\nWorking\nLower deprivation towns\nNo university\nMedium\n155.0\n71.42857143\n56.12903226\n83.87096774\n45.80645161\n35.483870967741936\n10.967741935483872\n7.741935483870968\n57.41935483870968\n27.741935483870968\n*\n*\n34.4\n31.2\n32.5\n3.54838710\n-1.04412799\n\n\nE34000026\nMarket Weighton BUA\n6429.0\nSmall Towns\nYorkshire and The Humber\nNon-coastal\nSmaller non-coastal town\nE30000220\nHull\nMajority town and city (Large Towns)\nResidential\nLower deprivation towns\nNo university\nMedium\n58.0\n70.96774194\n53.44827586\n91.22807018\n49.12280702\n25.862068965517242\n25.862068965517242\n*\n58.620689655172406\n31.03448275862069\n*\n*\n*\n66.1\n*\n3.48275862\n-1.24926195\n\n\nE34000027\nDownham Market BUA\n10884.0\nSmall Towns\nEast of England\nNon-coastal\nSmaller non-coastal town\nE30000225\nKing’s Lynn\nMajority rural\nMixed\nHigher deprivation towns\nNo university\nLow\n93.0\n78.04878049\n62.36559140\n78.49462366\n40.86021505\n26.881720430107524\n20.43010752688172\n15.053763440860216\n55.91397849462365\n30.107526881720432\n15.053763440860216\n*\n32.6\n44.2\n*\n3.16129032\n-1.16907845\n\n\nE34000039\nPenrith BUA\n15181.0\nSmall Towns\nNorth West\nNon-coastal\nSmaller non-coastal town\nE30000106\nPenrith\nMajority rural\nWorking\nLower deprivation towns\nNo university\nLow\n158.0\n73.85620915\n74.05063291\n91.13924051\n46.20253165\n31.645569620253166\n16.455696202531644\n12.658227848101266\n62.0253164556962\n37.34177215189873\n7.59493670886076\n*\n29.4\n40.0\n28.1\n3.47468354\n0.84513109\n\n\nE34000048\nBolsover BUA\n11754.0\nSmall Towns\nEast Midlands\nNon-coastal\nSmaller non-coastal town\nE30000190\nChesterfield\nMajority town and city (Large Towns)\nResidential\nHigher deprivation towns\nNo university\nLow\n118.0\n70.90909091\n50.00000000\n76.27118644\n37.28813559\n21.1864406779661\n27.11864406779661\n20.33898305084746\n50.847457627118644\n26.27118644067797\n11.864406779661017\n*\n34.5\n47.9\n*\n3.10169492\n-3.71791969\n\n\nE34000055\nMarch BUA\n21051.0\nMedium Towns\nEast of England\nNon-coastal\nLarge non-coastal town\nE30000287\nWisbech\nMajority town and city (small)\nResidential\nHigher deprivation towns\nNo university\nLow\n235.0\n71.06382979\n57.44680851\n84.25531915\n40.42553191\n25.53191489361702\n17.4468085106383\n10.212765957446807\n47.65957446808511\n26.80851063829787\n6.808510638297872\n*\n37.8\n37.8\n23.6\n3.28510638\n-2.17130716\n\n\nE34000056\nSoutham BUA\n6567.0\nSmall Towns\nWest Midlands\nNon-coastal\nSmaller non-coastal town\nE30000228\nLeamington Spa\nMajority town and city (small)\nWorking\nLower deprivation towns\nNo university\nMedium\n92.0\n81.92771084\n91.30434783\n90.21739130\n67.39130435\n38.04347826086957\n21.73913043478261\n16.304347826086957\n52.17391304347826\n25.0\n*\n*\n19.6\n47.8\n32.6\n3.84782609\n6.43766140\n\n\nE34000067\nRoyston BUA\n15781.0\nSmall Towns\nEast of England\nNon-coastal\nSmaller non-coastal town\nE30000186\nCambridge\nMajority town and city (Large Towns)\nWorking\nLower deprivation towns\nNo university\nMedium\n169.0\n73.03370787\n65.08875740\n82.24852071\n53.84615385\n23.668639053254438\n21.893491124260358\n14.201183431952662\n57.98816568047337\n39.053254437869825\n*\n*\n23.7\n50.9\n23.7\n3.38461538\n0.29540337\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\nE35001501\nReading BUASD\n218705.0\nLarge Towns\nSouth East\nNon-coastal\nLarge non-coastal town\nE30000256\nReading\nMajority town and city (Large Towns)\nWorking\nMid deprivation towns\nUniversity\nMedium\n2243.0\n73.81263617\n62.50557289\n82.65715560\n52.07311636\n37.672759696834596\n16.272848863129738\n9.80829246544806\n48.14979937583593\n24.83281319661168\n5.21622826571556\n2.4\n25.7\n42.5\n29.5\n3.53811859\n0.39981293\n\n\nE35001502\nHedge End BUASD\n25117.0\nMedium Towns\nSouth East\nNon-coastal\nLarge non-coastal town\nE30000267\nSouthampton\nMajority town and city (Large Towns)\nMixed\nLower deprivation towns\nNo university\nMedium\n307.0\n77.63578275\n70.68403909\n84.03908795\n58.30618893\n34.20195439739413\n19.218241042345277\n14.65798045602606\n60.91205211726385\n35.50488599348534\n5.537459283387622\n*\n18.1\n51.5\n29.1\n3.63517915\n2.48698423\n\n\nE35001503\nWestergate BUASD\n9783.0\nSmall Towns\nSouth East\nNon-coastal\nSmaller non-coastal town\nE30000191\nChichester and Bognor Regis\nMajority town and city (small)\nMixed\nLower deprivation towns\nNo university\nMedium\n108.0\n80.00000000\n62.96296296\n88.88888889\n56.48148148\n27.77777777777778\n14.814814814814813\n*\n54.629629629629626\n25.0\n*\n*\n22.9\n48.6\n27.5\n3.53703704\n1.56425938\n\n\nE35001507\nLoughton BUASD\n31106.0\nMedium Towns\nEast of England\nNon-coastal\nLarge non-coastal town\nE30000234\nLondon\nMajority conurbation\nWorking\nMid deprivation towns\nNo university\nMedium\n357.0\n75.55555556\n70.30812325\n84.83146067\n49.71910112\n35.01400560224089\n16.5266106442577\n10.92436974789916\n42.857142857142854\n28.57142857142857\n4.481792717086835\n*\n22.9\n46.4\n29.3\n3.57422969\n1.26655373\n\n\nE35001516\nMilton Keynes BUASD\n171750.0\nLarge Towns\nSouth East\nNon-coastal\nLarge non-coastal town\nE30000243\nMilton Keynes\nMajority town and city (Large Towns)\nWorking\nMid deprivation towns\nUniversity\nMedium\n2106.0\n70.43314501\n62.67806268\n85.99240266\n52.08926876\n42.26020892687559\n13.437796771130103\n6.837606837606838\n43.49477682811016\n21.225071225071225\n6.172839506172839\n1.9\n26.4\n38.9\n32.8\n3.65337132\n0.34182404\n\n\nE35001517\nRedditch BUASD\n81919.0\nLarge Towns\nWest Midlands\nNon-coastal\nLarge non-coastal town\nE30000169\nBirmingham\nMajority conurbation\nWorking\nHigher deprivation towns\nNo university\nLow\n1006.0\n67.58349705\n68.09145129\n87.17693837\n48.40954274\n32.30616302186879\n18.78727634194831\n9.642147117296222\n50.0\n22.962226640159045\n8.846918489065606\n1.7\n29.7\n41.3\n27.3\n3.47117296\n-0.29414288\n\n\nK06000004\nChester BUASD\n86011.0\nLarge Towns\nNorth West\nNon-coastal\nLarge non-coastal town\nK01000011\nChester\nMajority town and city (Large Towns)\nWorking\nHigher deprivation towns\nUniversity\nMedium\n739.0\n70.72192513\n61.02841678\n80.62330623\n46.20596206\n32.47631935047362\n19.485791610284167\n9.066305818673884\n45.33152909336942\n18.26792963464141\n7.983761840324763\n2.0\n27.3\n43.0\n27.7\n3.47496617\n-0.81093526\n\n\n\nInner London BUAs\n\nInner London BUA\nLondon\n\n\n\n\n\n\n\n\n\n24630.0\n69.91000918\n63.21965083\n84.88381540\n53.47335067\n49.26918392204628\n14.023548518067397\n5.952090946000812\n30.07308160779537\n10.117742590336988\n7.182298010556232\n1.9\n19.0\n44.0\n35.1\n3.78457166\n0.06759137\n\n\n\nOuter london BUAs\n\nOuter london BUA\nLondon\n\n\n\n\n\n\n\n\n\n52998.0\n74.51446320\n66.58364467\n86.60700800\n57.05331521\n48.47352730291709\n14.249594324314124\n7.462545756443639\n36.38816559115438\n15.551530246424395\n5.520963055209631\n1.7\n18.8\n42.5\n37.0\n3.86403261\n1.26241027\n\n\n\nOther Small BUAs\n\nOther Small BUAs\n\n\n\n\n\n\n\n\n\n\n59743.0\n77.35164037\n65.12562141\n86.39697264\n54.48745856\n37.30144117302445\n18.96623872252816\n12.269219824916727\n50.26530304805584\n24.33757929799307\n5.4985521316304835\n1.7\n23.3\n44.9\n30.1\n3.65651206\n1.22187679\n\n\n\nNot BUA\n\nNot BUA\n\n\n\n\n\n\n\n\n\n\n21765.0\n78.23469971\n67.50746612\n87.83783784\n57.17503218\n38.98460831610384\n18.111647139903514\n11.504709395818976\n48.86744773719274\n23.06914771422008\n4.741557546519641\n1.5\n21.2\n45.1\n32.2\n3.70797151\n1.80208942\n\n\n\n\n\n(hanami/plot english-education\n             ht/point-chart\n             {:X :education_score\n              :Y :size_flag\n              :YTYPE \"nominal\"})\n\n\n\n\n\n\n(hanami/plot english-education\n             ;; This is not ideal.. one of my many projects this year is to think up ways\n             ;; to improve Clojure's dataviz wrappers\n             (-&gt; ht/point-chart\n                 (assoc :encoding (assoc (hc/get-default :ENCODING)\n                                         :yOffset :YOFFSET)))\n             {:X :education_score\n              :Y :size_flag\n              :HEIGHT {:step 60}\n              :TRANSFORM [{:calculate\n                           ;; Generate Gaussian jitter with a Box-Muller transform, we could\n                           ;; also use `random()`, but that jitters the points uniformly and\n                           ;; looks worse\n                           \"sqrt(-2*log(random()))*cos(2*PI*random())\"\n                           :as \"jitter\"}]\n              :YTYPE \"nominal\"\n              :YOFFSET {:field \"jitter\" :type \"quantitative\"}})",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4 - Educational Attainment of Young People in English Towns</span>"
    ]
  },
  {
    "objectID": "year_2024.week_4.analysis.html#vega-spec-for-a-beeswarm-graph",
    "href": "year_2024.week_4.analysis.html#vega-spec-for-a-beeswarm-graph",
    "title": "Week 4 - Educational Attainment of Young People in English Towns",
    "section": "Vega spec for a beeswarm graph",
    "text": "Vega spec for a beeswarm graph\n\nGetting a tablecloth dataset into vega\nThe first step is getting our data to render in a vega spec through Clojure. For this we can use Daniel Slutsky’s amazing library kindly. It tells our namespace in a notebook-agnostic way how to render different things. If we pass a valid vega spec to kind/vega, our notebook will render it properly as a graph. So cool.\nSo first, to just get any data rendering in our graph, we’ll read (or in Clojure, slurp) our data from the relevant file. We have to specify the format since csv is not the default. For now we’ll just make a simple scatterplot to get the data on the page.\n\n(kind/vega\n  ;; every dataset in vega needs to be named, this is how we reference the\n  ;; data in the rest of the spec\n {:data [{:name \"english-education\"\n          :values (slurp \"data/year_2024/week_4/english_education.csv\")\n          :format {:type \"csv\"}}]\n  :width 500\n  :height 500\n  ;; the minimal vega spec includes scales, marks, and axes\n  ;; scales map data values to visual values, defining the nature of visual encodings\n  :scales [{:name \"yscale\"\n            :type \"band\"\n            :domain {:data \"english-education\" :field \"size_flag\"}\n            :range \"height\"}\n           {:name \"xscale\"\n            :domain [-12 12]\n            :range \"width\"}]\n  ;; axes label and provide context for the scales\n  :axes [{:orient \"bottom\" :scale \"xscale\"}\n         {:orient \"left\" :scale \"yscale\"}]\n  ;; marks map visual values to shapes on the screen\n  :marks [{:type \"symbol\"\n           :from {:data \"english-education\"}\n           :encode {:enter {:y {:field \"size_flag\"\n                                :scale \"yscale\"}\n                            :x {:field \"education_score\"\n                                :scale \"xscale\"}}}}]})\n\n\n\n\nThis is the minimal reproduction of the vega-lite scatterplot we made above. This is where we’ll dive deeper into vega to do some things vega-lite can’t do. In order to turn this into a beeswarm plot, we can add what vega calls forces.\n\n\nTurning a scatterplot into a beeswarm graph in vega\nForce transforms compute force-directed layouts, so we can use one to compute the placement of each point in our graph such that they cluster together based on their y value but don’t overlap. To compute a beeswarm layout we’ll tell vega to treat each point like it’s attracted to other ones that share its y value but not to collide with them.\n\n(kind/vega\n {:data [{:name \"english-education\"\n          :values (slurp \"data/year_2024/week_4/english_education.csv\")\n          :format {:type \"csv\"}}]\n  :width 800\n  :height 800\n  :scales [{:name \"yscale\"\n            :type \"band\"\n            :domain {:data \"english-education\" :field \"size_flag\"}\n            :range \"height\"}\n           {:name \"xscale\"\n            :domain [-12 12]\n            :range \"width\"}]\n  :axes [{:orient \"bottom\" :scale \"xscale\"}\n         {:orient \"left\" :scale \"yscale\"}]\n  :marks [{:type \"symbol\"\n           :from {:data \"english-education\"}\n           :encode {:enter {;; we update the mark encoding to use the x and y values\n                            ;; computed by the force transform\n                            :yfocus {:field \"size_flag\"\n                                     :scale \"yscale\"\n                                     :band 0.5}\n                            :xfocus {:field \"education_score\"\n                                     :scale \"xscale\"\n                                     :band 0.5}}}\n           ;; this is the new part -- a simulated force attracting the points to each other\n           :transform [{:type \"force\"\n                        :static true\n                        :forces [{:force \"x\" :x \"xfocus\"}\n                                 {:force \"y\" :y \"yfocus\"}\n                                 {:force \"collide\" :radius 4}]}]}]})\n\n\n\n\nThis is pretty close to the graph we’re going for! We can add some more aesthetic details to make it look neater, like changing the definition of the points and adding a grid along the x axis:\n\n(kind/vega\n {:data [{:name \"english-education\"\n          :values (slurp \"data/year_2024/week_4/english_education.csv\")\n          :format {:type \"csv\"}}]\n  :width 800\n  :height 800\n  :scales [{:name \"yscale\"\n            :type \"band\"\n            :domain {:data \"english-education\" :field \"size_flag\"}\n            :range \"height\"}\n           {:name \"xscale\"\n            :domain [-12 12]\n            :range \"width\"}]\n  :axes [{:orient \"bottom\" :scale \"xscale\" :grid true}\n         {:orient \"left\" :scale \"yscale\"}]\n  :marks [{:type \"symbol\"\n           :from {:data \"english-education\"}\n           :encode {:enter {:yfocus {:field \"size_flag\"\n                                     :scale \"yscale\"\n                                     :band 0.5}\n                            :xfocus {:field \"education_score\"\n                                     :scale \"xscale\"\n                                     :band 0.5}\n                            :fill {:value \"skyblue\"}\n                            :stroke {:value \"white\"}\n                            :strokeWidth {:value 1}\n                            :zindex {:value 0}}}\n           :transform [{:type \"force\"\n                        :static true\n                        :forces [{:force \"x\" :x \"xfocus\"}\n                                 {:force \"y\" :y \"yfocus\"}\n                                 {:force \"collide\" :radius 4}]}]}]})\n\n\n\n\nWoohoo. Making progress. Next we can add the average lines. We’ll transform the data we already have into a new grouped dataset with the average computed per size_flag, and add another mark layer on top to show these lines:\n\n(kind/vega\n {:data [{:name \"english-education\"\n          :values (slurp \"data/year_2024/week_4/english_education.csv\")\n          :format {:type \"csv\"}}\n         ;; this is the new dataset, computed from the existing one\n         {:name \"averages\"\n          :source \"english-education\"\n          :transform [{:type \"aggregate\",\n                       :fields [\"education_score\"],\n                       :groupby [\"size_flag\"],\n                       :ops [\"average\"],\n                       :as [\"avg\"]}]}]\n  :width 800\n  :height 800\n  :scales [{:name \"yscale\"\n            :type \"band\"\n            :domain {:data \"english-education\" :field \"size_flag\"}\n            :range \"height\"}\n           {:name \"xscale\"\n            :domain [-12 12]\n            :range \"width\"}]\n  :axes [{:orient \"bottom\" :scale \"xscale\" :grid true :title \"Education score\"}\n         {:orient \"left\" :scale \"yscale\" :title \"Town size\"}]\n  :marks [{:type \"symbol\"\n           :from {:data \"english-education\"}\n           :encode {:enter {:yfocus {:field \"size_flag\"\n                                     :scale \"yscale\"\n                                     :band 0.5}\n                            :xfocus {:field \"education_score\"\n                                     :scale \"xscale\"\n                                     :band 0.5}\n                            :fill {:value \"skyblue\"}\n                            :stroke {:value \"white\"}\n                            :strokeWidth {:value 1}\n                            :zindex {:value 0}}}\n           :transform [{:type \"force\"\n                        :static true\n                        :forces [{:force \"x\" :x \"xfocus\"}\n                                 {:force \"y\" :y \"yfocus\"}\n                                 {:force \"collide\" :radius 4}]}]}\n          ;; this is the new layer with the lines -- it's second so that the lines show up on top\n          {:type \"symbol\"\n           ;; we tell it to use the data from our new, computed dataset\n           :from {:data \"averages\"}\n           :encode {:enter {:x {:field \"avg\" :scale \"xscale\"}\n                            :y {:field \"size_flag\" :scale \"yscale\" :band 0.5}\n                            :shape {:value \"stroke\"}\n                            :strokeWidth {:value 1.5}\n                            :stroke {:value \"black\"}\n                            :size {:value 14000}\n                            :angle {:value 90}}}}]})\n\n\n\n\nWe can add the town selection dropdown like in the example graph using vega signals.\n\n(let [town-name-values (-&gt; english-education\n                           (tc/select-columns [:town11nm])\n                           tc/rows\n                           flatten\n                           sort\n                           (conj \"\"))]\n  (kind/vega\n   {:data [{:name \"english-education\"\n            :values (slurp \"data/year_2024/week_4/english_education.csv\")\n            :format {:type \"csv\"}}\n           {:name \"averages\"\n            :source \"english-education\"\n            :transform [{:type \"aggregate\",\n                         :fields [\"education_score\"],\n                         :groupby [\"size_flag\"],\n                         :ops [\"average\"],\n                         :as [\"avg\"]}]}]\n    :width 800\n    :height 800\n    :scales [{:name \"yscale\"\n              :type \"band\"\n              :domain {:data \"english-education\" :field \"size_flag\"}\n              :range \"height\"}\n             {:name \"xscale\"\n              :domain [-12 12]\n              :range \"width\"}]\n    :axes [{:orient \"bottom\" :scale \"xscale\" :grid true :title \"Education score\"}\n           {:orient \"left\" :scale \"yscale\" :title \"Town size\"}]\n    :signals [{:name \"highlight\"\n               :value nil\n               :bind {:input :select\n                      :options town-name-values\n                      :labels (map #(str/replace % #\" BUAS?D?\" \"\") town-name-values)}}]\n    :marks [{:type \"symbol\"\n             :from {:data \"english-education\"}\n             :encode {:enter {:yfocus {:field \"size_flag\"\n                                       :scale \"yscale\"\n                                       :band 0.5}\n                              :xfocus {:field \"education_score\"\n                                       :scale \"xscale\"\n                                       :band 0.5}\n                              :fill {:value \"skyblue\"}\n                              :stroke {:value \"white\"}\n                              :strokeWidth {:value 1}\n                              :zindex {:value 0}}\n                      :update {:fill {:signal \"datum['town11nm'] === highlight ? 'orange' : 'skyblue'\"}\n                               :stroke {:signal \"datum['town11nm'] === highlight ? 'purple' : 'white'\"}\n                               :strokeWidth {:signal \"datum['town11nm'] === highlight ? 2 : 1\"}\n                               :zindex {:signal \"datum['town11nm'] === highlight ? 1 : 0\"}}}\n             :transform [{:type \"force\"\n                          :static true\n                          :forces [{:force \"x\" :x \"xfocus\"}\n                                   {:force \"y\" :y \"yfocus\"}\n                                   {:force \"collide\" :radius 4}]}]}\n            {:type \"symbol\"\n             :from {:data \"averages\"}\n             :encode {:enter {:x {:field \"avg\" :scale \"xscale\"}\n                              :y {:field \"size_flag\" :scale \"yscale\" :band 0.5}\n                              :shape {:value \"stroke\"}\n                              :strokeWidth {:value 1.5}\n                              :stroke {:value \"black\"}\n                              :size {:value 14000}\n                              :angle {:value 90}}}}]}))\n\n\n\n\nThe select dropdown isn’t super nice or in a great spot. The way to style and position it better is with CSS, but I’m going to call that out of scope for this exercise. In theory, this dropdown would be above the chart and look much nicer. This is great, though. We have more or less re-created the main graphic from the article. We can see that smaller towns seem to have higher educational attainment on average, and check the value for any given town by selecting its value from a dropdown.\nOne of the many projects I’d like to tackle is making a wrapper for vega to make it intuitive to use from Clojure. Anyway, moving on to the next graph in the article.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4 - Educational Attainment of Young People in English Towns</span>"
    ]
  },
  {
    "objectID": "year_2024.week_4.analysis.html#income-deprivation-and-town-size",
    "href": "year_2024.week_4.analysis.html#income-deprivation-and-town-size",
    "title": "Week 4 - Educational Attainment of Young People in English Towns",
    "section": "Income deprivation and town size",
    "text": "Income deprivation and town size\n\nNormalized, stacked bar chart\nThe next one is a simple normalized stacked bar chart exploring the relationship between town size and income deprivation. For this one we can drop back into vega-lite (and hanami):\n\n(-&gt; english-education\n    (hanami/plot ht/bar-chart\n                 {:XAGG \"sum\"\n                  :XSTACK \"normalize\"\n                  :XTITLE \"Count\"\n                  :X :education_score\n                  :YTYPE \"nominal\"\n                  :Y :size_flag\n                  :YTITLE \"Town size\"\n                  :COLOR \"income_flag\"\n                  :CTYPE \"nominal\"}))\n\n\n\n\nThe data from the “BUA”s and London is making this look less than ideal, so we’ll filter for only the categories of towns that are included in the article’s graph:\n\n(-&gt; english-education\n    (tc/select-rows #(#{\"Small Towns\" \"Medium Towns\" \"Large Towns\" \"City\"}\n                       (:size_flag %)))\n    (hanami/plot ht/bar-chart\n                 {:XAGG \"sum\"\n                  :XSTACK \"normalize\"\n                  :XTITLE \"Count\"\n                  :X :education_score\n                  :YTYPE \"nominal\"\n                  :Y :size_flag\n                  :YTITLE \"Town size\"\n                  :COLOR \"income_flag\"\n                  :CTYPE \"nominal\"}))\n\n\n\n\nThis seems to show that the incidence of income deprivation is higher the larger the town size. We can visualize this relationship in another way by making a scatterplot, like in the article.\n\n\nScatterplot\nFor this graph we’ll use the dataset provided in the article (since it contains income deprivation scores, not just classifications).\n\n(-&gt; \"data/year_2024/week_4/education-and-income-scores.csv\"\n    tc/dataset\n    (hanami/plot ht/point-chart\n                 {:X \"Educational attainment score\"\n                  :Y \"Income deprivation score\"\n                  :YSCALE {:zero false}}))\n\n\n\n\nThis reveals that income deprivation seems to be worse in larger towns, but this isn’t the whole story. The article also investigates regional differences. To see the effect of region on education scores, we can plot scores vs region and encode the town size in the color of the points. We’ll go back to our original dataset for this. The rgn11nm column contains the region we’re looking for.\n\n\nEducation score by region\n\n(-&gt; english-education\n    (hanami/plot ht/point-chart\n                 {:X :education_score\n                  :Y :rgn11nm\n                  :YTYPE \"nominal\"\n                  :COLOR \"income_flag\"\n                  :CTYPE \"nominal\"}))\n\n\n\n\nThis is pretty close, but we can take the average of all scores in a given region to reduce some of the noise, and make the points larger to make it easier to see them.\n\n(-&gt; english-education\n    (hanami/plot ht/point-chart\n                 {:X :education_score\n                  :XAGG \"mean\"\n                  :Y :rgn11nm\n                  :YTYPE \"nominal\"\n                  :MSIZE 100\n                  :COLOR \"income_flag\"\n                  :CTYPE \"nominal\"}))\n\n\n\n\nThis reveals an interesting relationship between region and education scores. The North West has the highest education scores at all levels of income deprivation.\n\n\nEffect of being near the coast\nThe next graph is an interesting one showing that coastal towns have worse outcomes than non-coastal towns. The graph is similar to the previous one, but in order to get the data into a sensible shape to visualize I’m going to wrangle it a bit ahead of throwing it into our viz function. We have the coastal_detailed column that gives us, well, details about the coastal-ness of each town. Inspecting the distinct values in this column shows us it’s a bit of a mess, though.\n\n(-&gt; english-education :coastal_detailed vec distinct)\n\n\n(\"Smaller non-coastal town\"\n \"Large non-coastal town\"\n \"Smaller seaside town\"\n \"Large seaside town\"\n \"Smaller other coastal town\"\n \"Large other coastal town\"\n \"Cities\"\n nil)\n\nThis one column contains strings that describe a town as either “smaller” or “large”, and “non-coastal”, “seaside”, or “other coastal”. The first thing we’ll do is tidy up this data. Each variable should be in its own column, plus we’ll delete all the rows that don’t have a value for coastal_detailed. Then we’ll pass it to our viz function, taking the average of the education score for each category we’re plotting, just like above.\n\n(-&gt; english-education\n    (tc/select-columns [:education_score :coastal_detailed])\n    (tc/drop-missing :coastal_detailed)\n    (tc/map-columns :town_size [:coastal_detailed]\n                    (fn [v]\n                      (when v\n                        (if (str/includes? v \"Smaller\") \"Small\" \"Large\"))))\n    (tc/map-columns :coastal_type [:coastal_detailed]\n                    (fn [v]\n                      (when v\n                        (cond\n                          (str/includes? v \"non-coastal\") \"Non-coastal\"\n                          (str/includes? v \"seaside\") \"Seaside\"\n                          :else \"Other coastal\"))))\n    (hanami/plot ht/point-chart\n                 {:X :education_score\n                  :XAGG \"mean\"\n                  :Y :town_size\n                  :YTYPE \"nominal\"\n                  :COLOR \"coastal_type\"\n                  :CTYPE \"nominal\"\n                  :MSIZE 100\n                  :HEIGHT 100}))\n\n\n\n\nThis reveals in interesting relationship between the proximity of a town to the coast and education scores. For whatever reason, inland towns have better educational attainment.\n\n\nWidening educational attainment gap over time\nThe article also observes that the gap in educational attainment between students from high vs low income deprivation areas widens over time. We can see this in the data by finding the average “key stage 2” (end of primary school in the UK) attainment in a given income deprivation category and comparing it to two later measures of educational attainment (key stage 4 and level 3).\nI’ll define a function for computing the average of a sequence of numbers, for the sake of clarifying the data transformation.\n\n(defn- average [vals]\n  (/ (reduce + vals) (count vals)))\n\n\n#'year-2024.week-4.analysis/average\n\nWe’ll also filter out rows that have “Cities” and nil as their income_flag because these aren’t comparable to the other income flag values.\n\n(-&gt; english-education\n    (tc/drop-missing :income_flag)\n    (tc/drop-rows #(= \"Cities\" (:income_flag %)))\n    (tc/group-by [:income_flag])\n    (tc/aggregate-columns [:key_stage_2_attainment_school_year_2007_to_2008\n                           :key_stage_4_attainment_school_year_2012_to_2013\n                           :level_3_at_age_18]\n                          average))\n\n_unnamed [3 4]:\n\n\n\n\n\n\n\n\n\n:income_flag\n:key_stage_2_attainment_school_year_2007_to_2008\n:key_stage_4_attainment_school_year_2012_to_2013\n:level_3_at_age_18\n\n\n\n\nHigher deprivation towns\n69.46241954\n55.61679938\n41.59478974\n\n\nMid deprivation towns\n72.95656558\n59.34720275\n47.82498013\n\n\nLower deprivation towns\n79.29026824\n67.95547806\n57.96412983\n\n\n\nWe’ll calculate the gap in attainment between the different deprivation categories. I’m going to rename the columns first to make them more succinct. Then we’ll add new columns that show the gap between each stage for a given income deprivation level (compared to the lowest deprivation category).\n\n(let [ds (-&gt; english-education\n             (tc/drop-missing :income_flag)\n             (tc/drop-rows #(= \"Cities\" (:income_flag %)))\n             (tc/group-by [:income_flag])\n             (tc/aggregate-columns [:key_stage_2_attainment_school_year_2007_to_2008\n                                    :key_stage_4_attainment_school_year_2012_to_2013\n                                    :level_3_at_age_18]\n                                   average)\n             (tc/rename-columns {:key_stage_2_attainment_school_year_2007_to_2008 :key_stage_2\n                                 :key_stage_4_attainment_school_year_2012_to_2013 :key_stage_4\n                                 :level_3_at_age_18 :level_3}))\n      lowest-deprivation-vals (tc/select-rows ds #(str/includes? (:income_flag %) \"Lower\"))]\n  (-&gt; ds\n      (tc/map-columns :key_stage_2_gap [:key_stage_2]\n                      #(- % (first (:key_stage_2 lowest-deprivation-vals))))\n      (tc/map-columns :key_stage_4_gap [:key_stage_4]\n                      #(- % (first (:key_stage_4 lowest-deprivation-vals))))\n      (tc/map-columns :level_3_gap [:level_3]\n                      #(- % (first (:level_3 lowest-deprivation-vals))))))\n\n_unnamed [3 7]:\n\n\n\n\n\n\n\n\n\n\n\n\n:income_flag\n:key_stage_2\n:key_stage_4\n:level_3\n:key_stage_2_gap\n:key_stage_4_gap\n:level_3_gap\n\n\n\n\nHigher deprivation towns\n69.46241954\n55.61679938\n41.59478974\n-9.82784870\n-12.33867868\n-16.36934009\n\n\nMid deprivation towns\n72.95656558\n59.34720275\n47.82498013\n-6.33370266\n-8.60827531\n-10.13914970\n\n\nLower deprivation towns\n79.29026824\n67.95547806\n57.96412983\n0.00000000\n0.00000000\n0.00000000\n\n\n\nThis data is already pretty easy to interpret just as a table. For the sake of the exercise, we can plot it in a similar way to the article. To do this we’ll tidy up the data in a different way, in all senses of the word. Right now this data is structured in a way that makes it easy to interpret when it’s printed as a table, but it’s hard to plot because some of the column names encode a variable in our dataset across multiple columns (the education attainment stage). To fix this, we’ll use tablecloth’s pivot-&gt;longer, so that each row represents one observation and each column represents a single variable. Then we can plot it.\n\n(let [ds (-&gt; english-education\n             (tc/drop-missing :income_flag)\n             (tc/drop-rows #(= \"Cities\" (:income_flag %)))\n             (tc/group-by [:income_flag])\n             (tc/aggregate-columns [:key_stage_2_attainment_school_year_2007_to_2008\n                                    :key_stage_4_attainment_school_year_2012_to_2013\n                                    :level_3_at_age_18]\n                                   average)\n             (tc/rename-columns {:key_stage_2_attainment_school_year_2007_to_2008 :key_stage_2\n                                 :key_stage_4_attainment_school_year_2012_to_2013 :key_stage_4\n                                 :level_3_at_age_18 :level_3}))\n      lowest-deprivation-vals (tc/select-rows ds #(str/includes? (:income_flag %) \"Lower\"))]\n  (-&gt; ds\n      (tc/map-columns :key_stage_2_gap [:key_stage_2]\n                      #(- % (first (:key_stage_2 lowest-deprivation-vals))))\n      (tc/map-columns :key_stage_4_gap [:key_stage_4]\n                      #(- % (first (:key_stage_4 lowest-deprivation-vals))))\n      (tc/map-columns :level_3_gap [:level_3]\n                      #(- % (first (:level_3 lowest-deprivation-vals))))\n      (tc/select-columns [:income_flag :key_stage_2_gap :key_stage_4_gap :level_3_gap])\n      (tc/pivot-&gt;longer [:key_stage_2_gap :key_stage_4_gap :level_3_gap]\n                        {:target-columns :gap\n                         :value-column-name :value})\n      (hanami/plot ht/point-chart\n                   {:X :gap\n                    :XTYPE \"nominal\"\n                    :Y :value\n                    :COLOR \"income_flag\"\n                    :CTYPE \"nominal\"\n                    :MSIZE 100})))\n\n\n\n\nThis could be tidied up a bit with nicer names and colours, but this is the idea. I think the main takeaway from this graph is that data should be tidy for the sake of visualizing it, and that naming columns is important.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4 - Educational Attainment of Young People in English Towns</span>"
    ]
  },
  {
    "objectID": "year_2024.week_4.analysis.html#pursuit-of-higher-education",
    "href": "year_2024.week_4.analysis.html#pursuit-of-higher-education",
    "title": "Week 4 - Educational Attainment of Young People in English Towns",
    "section": "Pursuit of higher education",
    "text": "Pursuit of higher education\nThe next question we can answer with this data is how many students from different income deprivation areas pursue higher education. This chart looks complex but it’s just a simple bar chart faceted by educational attainment milestones. We have the information to make this chart, but again it’s not organized in a nice way for visualizing.\nThe data we care about for this chart are the town size flag, and the three educational attainment values in the columns level_3_at_age_18, activity_at_age_19:_full-time_higher_education, and activity_at_age_19:_appprenticeships. We’ll pivot our data again in a different way this time to organize the data for this visualization, then aggregate the values to calculate the average value for each town size/attainment level pair. As an added bonus, the data in the activity... columns are strings, not numbers, so we’ll coerce those to numbers too so that we can do math on them (like calculating the average).\n\n(-&gt; english-education\n    (tc/select-columns [:size_flag\n                        :level_3_at_age_18\n                        :activity_at_age_19:_full-time_higher_education\n                        :activity_at_age_19:_sustained_further_education])\n    (tc/rename-columns {:level_3_at_age_18 \"Level 3 qualifications at age 18\"\n                        :activity_at_age_19:_full-time_higher_education \"In full-time higher education at age 19\"\n                        :activity_at_age_19:_sustained_further_education  \"In further education at age 19\"})\n    (tc/pivot-&gt;longer [\"Level 3 qualifications at age 18\"\n                       \"In full-time higher education at age 19\"\n                       \"In further education at age 19\"]\n                      {:target-columns :attainment_measure\n                       :value-column-name :value})\n    ;; coerce all the values to be numbers\n    (tc/update-columns :value (partial map #(if (string? %) (parse-double %) %)))\n    ;; lose the nil ones, it messes up our calculation\n    (tc/drop-missing :value)\n    ;; calculate the average value for each size flag/attainment measure pair\n    (tc/group-by [:size_flag :attainment_measure])\n    (tc/aggregate-columns :value average)\n    ;; plot this\n    (hanami/plot {:facet :FACET\n                  :spec {:encoding :ENCODING\n                         :layer :LAYER\n                         :width :WIDTH}\n                  :data {:values :DATA\n                         :format :DFMT}}\n                 {:Y :size_flag\n                  :YTITLE \"Town size\"\n                  :YTYPE \"nominal\"\n                  :YSORT [\"Small towns\"\n                          \"Medium towns\"\n                          \"Large towns\"\n                          \"Cities\"\n                          \"Outer London\"\n                          \"Inner London\"]\n                  :X :value\n                  :XTITLE \"Percentage of population\"\n                  :FACET {:column {:field :attainment_measure\n                                   :type \"nominal\"\n                                   :title \"Attainment measure\"\n                                   :sort [\"Level 3 qualifications at age 18\"\n                                          \"In full-time higher education at age 19\"\n                                          \"In further education at age 19\"]}}\n                  :LAYER [{:mark \"bar\"}\n                          {:mark {:type \"text\" :dx -3 :color \"white\" :align \"right\"}\n                           :encoding {:text {:field \"value\" :format \".1f\"}}}]\n                  :WIDTH 140}))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4 - Educational Attainment of Young People in English Towns</span>"
    ]
  },
  {
    "objectID": "year_2024.week_4.analysis.html#connection-to-other-town-residents",
    "href": "year_2024.week_4.analysis.html#connection-to-other-town-residents",
    "title": "Week 4 - Educational Attainment of Young People in English Towns",
    "section": "Connection to other town residents",
    "text": "Connection to other town residents\nThe last graph in the article is one showing the relationship between the educational attainment of older and younger residents. We can see that education scores seem to be correlated with the incidence of high educational attainment among older residents of a town. We don’t have exactly the right data to reproduce this graph, but we can do something similar with the data we do have.\nWe can plot the educational attainment values vs. the educational attainment classification of residents aged 35-64 and add some jitter to spread out the dots, which will reveal the same general relationship as the more precise data – students in towns with more highly educated older generations tend to have higher educational attainment. It’s still worth pointing out that there is a huge overlap between the highest low-education town educational attainment scores and the lowest high-education town scores, so there are obviously many other factors at play. But it’s still an interesting observation.\n\n(-&gt; english-education\n    (tc/drop-missing :level4qual_residents35-64_2011)\n    (hanami/plot {:encoding (assoc (hc/get-default :ENCODING)\n                                   :yOffset :YOFFSET)\n                  :mark {:type \"circle\"}\n                  :transform :TRANSFORM\n                  :height 300\n                  :width 500\n                  :data {:values :DATA\n                         :format :DFMT}}\n                 {:X :education_score\n                  :Y :level4qual_residents35-64_2011\n                  :YTYPE \"nominal\"\n                  :YSORT [\"High\" \"Medium\" \"Low\"]\n                  :YOFFSET {:field  \"jitter\" :type \"quantitative\"}\n                  :TRANSFORM [{:calculate \"sqrt(-2*log(random()))*cos(2*PI*random())\"\n                               :as \"jitter\"}]}))\n\n\n\n\nThat sums up our work this week. There are so many graphs in this one, it was a lot of fun to play around and see some common patterns emerging. I’m looking forward to many projects revolving around tidying up the dataviz story in Clojure.\nSee you next week :)\n\n\n\n\nsource: src/year_2024/week_4/analysis.clj",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4 - Educational Attainment of Young People in English Towns</span>"
    ]
  },
  {
    "objectID": "year_2024.week_5.analysis.html",
    "href": "year_2024.week_5.analysis.html",
    "title": "Week 5 - Groundhog Predictions",
    "section": "",
    "text": "Increasing popularity of Groundhog Day\nOne thing I quickly noticed when I started poking around the data is that for most of history there were very few groundhogs making predictions. We can visualize this increasing popularity of groundhog day in lots of ways. My first thought was to just make a simple line chart showing the numbers of new groundhogs reporting every year.\nFirst we load the data, making keywords keys as always.\nThen we have to wrangle the data a little bit to add each groundhog’s debut year to our dataset. We can do this by finding the earliest prediction for each groundhog, and joining this year to our groundhogs dataset.\nNow we have a dataset listing all the groundhogs that report on the weather that includes their first year in the business. We can count the number of new groundhogs reporting every year and plot this as a line:\nThis doesn’t really tell us much. It might be more easy to interpret as a cumulative area chart – with the area showing the cumulative count of groundhogs. One way to do this is directly in vega-lite, with a transform:\nBut in some cases, for example if the dataset were way bigger or if there were more complicated calculations to do than just summing up a field, we might want to manipulate the data on the JVM side and just send the points to plot to the browser. This is a good chance to demonstrate how to work with rolling windows in tablecloth too, so for the sake of demonstration, that would look like this:\nThat’s a bit better. It’s obvious from this that groundhog day really took off in the 2010s. We could show this more clearly by plotting the number of new groundhogs per decade:\nOver 30 places got groundhogs reporting on the weather in the 2010s. I wonder what happened then? I’m also curious about where these groundhogs are from.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5 - Groundhog Predictions</span>"
    ]
  },
  {
    "objectID": "year_2024.week_5.analysis.html#increasing-popularity-of-groundhog-day",
    "href": "year_2024.week_5.analysis.html#increasing-popularity-of-groundhog-day",
    "title": "Week 5 - Groundhog Predictions",
    "section": "",
    "text": "(def groundhogs\n  (-&gt; \"data/year_2024/week_5/groundhogs.csv\"\n      (tc/dataset {:key-fn keyword})))\n\n\n\n(def predictions\n  (-&gt; \"data/year_2024/week_5/predictions.csv\"\n      (tc/dataset {:key-fn keyword})))\n\n\n(def groundhogs-with-debut-year\n  (-&gt; predictions\n      (tc/group-by [:id])\n      ;; find the earliest year for each unique ID\n      (tc/aggregate {:first-year #(apply min (:year %))})\n      ;; add these to the groundhogs dataset\n      (tc/inner-join groundhogs :id)))\n\n\n\n(-&gt; groundhogs-with-debut-year\n    (tc/group-by [:first-year])\n    (tc/aggregate {:new-groundhogs tc/row-count})\n    (hanami/plot ht/line-chart\n                 {:X :first-year\n                  :XTITLE \"Year\"\n                  :XTYPE :temporal\n                  :Y :new-groundhogs\n                  :YTITLE \"Number of groundhogs debuting in this year\"}))\n\n\n\n\n\n\n(-&gt; groundhogs-with-debut-year\n    (tc/group-by [:first-year])\n    (tc/aggregate {:new-groundhogs tc/row-count})\n    (hanami/plot ht/area-chart\n                 {:X :first-year\n                  :XTITLE \"Year\"\n                  :XTYPE :temporal\n                  :Y :cumulative-count\n                  :YTITLE \"Cumulative count of groundhogs\"\n                  :TRANSFORM [{:sort [{:field :first-year}]\n                               :window [{:as :cumulative-count\n                                         :field :new-groundhogs\n                                         :op \"sum\"}]}]}))\n\n\n\n\n\n\n(-&gt; groundhogs-with-debut-year\n    (tc/group-by [:first-year])\n    (tc/aggregate {:new-groundhogs tc/row-count})\n    (rolling/expanding {:cumulative-count (rolling/sum :new-groundhogs)})\n    ;; now we have our years and cumulative count of groundhogs computed\n    (hanami/plot ht/area-chart\n                 {:X :first-year\n                  :XTYPE :temporal\n                  :Y :cumulative-count}))\n\n\n\n\n\n\n(-&gt; groundhogs-with-debut-year\n    ;; add a new column with the decade of each groundhog's first year\n    (tc/map-columns :decade [:first-year] #(-&gt; %\n                                               (/ 10)\n                                               (Math/floor)\n                                               (* 10)\n                                               int))\n    ;; count how many new groundhogs there were per decade\n    (tc/group-by [:decade])\n    (tc/aggregate {:count tc/row-count})\n    ;; make a simple bar chart with this data\n    (hanami/plot ht/bar-chart\n                 {:X :decade\n                  :XTYPE :temporal\n                  :MSIZE 15\n                  :Y :count}))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5 - Groundhog Predictions</span>"
    ]
  },
  {
    "objectID": "year_2024.week_5.analysis.html#geographic-distribution-of-groundhogs",
    "href": "year_2024.week_5.analysis.html#geographic-distribution-of-groundhogs",
    "title": "Week 5 - Groundhog Predictions",
    "section": "Geographic distribution of groundhogs",
    "text": "Geographic distribution of groundhogs\nTo see this we can show each groundhog’s location on a map. They’re all in the USA or Canada, so we can project these onto a map of those two countries. Vega-lite supports simple projections onto maps, so we’ll try our luck with it. We’ll have to write a custom hanami template, though, but that’s no problem.\n\n(-&gt; groundhogs-with-debut-year\n    (tc/map-columns :decade [:first-year] #(-&gt; %\n                                               (/ 10)\n                                               (Math/floor)\n                                               (* 10)\n                                               int))\n    (tc/select-columns [:id :decade :latitude :longitude])\n    (hanami/plot {:layer [{:data {:format {:type \"topojson\" :feature :default} :url :TOPOURL}\n                           :mark {:fill \"lightgray\" :type \"geoshape\"}\n                           :projection :PROJECTION}\n                          {:data {:values :DATA :format :DFMT}\n                           :encoding {:latitude {:field :LAT :type \"quantitative\"}\n                                      :longitude {:field :LONG :type \"quantitative\"}\n                                      :color :COLOR}\n                           :projection :PROJECTION\n                           :mark {:type \"circle\" :size :MSIZE}}]\n                  :height :HEIGHT\n                  :width :WIDTH}\n                 {:HEIGHT 1000\n                  :WIDTH 900\n                  :TOPOURL \"https://code.highcharts.com/mapdata/custom/usa-and-canada.topo.json\"\n                  :PROJECTION {:type \"conicConformal\"\n                               :parallels [45 33]\n                               :rotate [105]}\n                  :LAT \"latitude\"\n                  :LONG \"longitude\"\n                  :MSIZE 80\n                  :COLOR {:field \"decade\"\n                          :scale {:scheme \"category10\"}}}))\n\n\n\n\nThis shows us that Groundhog Day is much more widely observed in the north-east than anywhere else. I also can’t help but notice three dots in the province where I live, so I have to find out who my local groundhogs are.\n\n(-&gt; groundhogs-with-debut-year\n    (tc/select-rows #(= \"Canada\" (:country %)))\n    tc/row-count)\n\n\n14\n\nThere are 14 groundhogs in total in Canada. We can see just the ones in Nova Scotia:\n\n(-&gt; groundhogs-with-debut-year\n    (tc/select-rows #(= \"Nova Scotia\" (:region %))))\n\ninner-join [3 18]:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:id\n:first-year\n:description\n:slug\n:latitude\n:longitude\n:isGroundhog\n:name\n:city\n:type\n:source\n:region\n:active\n:shortname\n:image\n:currentPrediction\n:predictionsCount\n:country\n\n\n\n\n16\n1999\nShubenacadie Sam is a weather-predicting groundhog who resides at the Shubenacadie Wildlife Park in Nova Scotia. Living on the eastern edge of Canada, Sam makes the first prediction in North America every year at 8:00 am AST. Sam is active on Twitter and enjoys yoga and meditation.\nshubenacadie-sam\n45.0921321\n-63.3973390\ntrue\nShubenacadie Sam\nShubenacadie\nGroundhog\nhttps://data.novascotia.ca/Lands-Forests-and-Wildlife/Shubenacadie-Wildlife-Park-Groundhog-Day-Predictio/4y37-h2yu\nNova Scotia\ntrue\nSam\nhttps://groundhog-day.com/images/ghogs/shubenacadie-sam.jpeg\nhttps://twitter.com/ShubenacadieSam/status/1753395520351986102\n26\nCanada\n\n\n31\n2010\nTwo Rivers Tunnel is the official prognosticator for Cape Breton, located in Two Rivers Wildlife Park. Tunnel the groundhog lives in a charming green house and speaks in a distinct Groundhogese dialect — most likely native to Cape Breton, where he has been making predictions for over 20 years.\ntwo-rivers-tunnel\n45.9364617\n-60.3005507\ntrue\nTwo Rivers Tunnel\nCape Breton\nGroundhog\nhttps://www.facebook.com/TwoRiversWildlifePark/\nNova Scotia\ntrue\nTunnel\nhttps://groundhog-day.com/images/ghogs/two-rivers-tunnel.jpeg\nhttps://www.facebook.com/TwoRiversWildlifePark/posts/pfbid0wdbQg47T3HArkyF2LvRMtEfGsCj4arPPKuHWhANQWYPbkFp9qm6zmXmpyf8xmy3fl\n15\nCanada\n\n\n63\n2018\nLucy the Lobster makes Groundhog Day predictions on February 2nd to kick off the Lobster Crawl festival in Barrington, Nova Scotia. The Municipality of Barrington says Lucy is the only prognosticating crustacean to make a prediction in Canada. Bill Murray was reported as being bemused by the idea of a lobster forecaster.\nlucy-the-lobster\n43.5128603\n-65.6113970\nfalse\nLucy the Lobster\nBarrington\nAtlantic lobster\nhttps://twitter.com/lucylobsterns\nNova Scotia\ntrue\nLucy\nhttps://groundhog-day.com/images/ghogs/lucy-the-lobster.jpeg\nhttps://twitter.com/NateTWN/status/1753389360357978209\n6\nCanada\n\n\n\nIt turns out the one closest to me is actually a lobster! Which is adorable. Lobster fishing is a huge industry in my area, so this is a great gimmick. Anyway, moving on.\nI think it would be interesting to explore the predictions a little bit too.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5 - Groundhog Predictions</span>"
    ]
  },
  {
    "objectID": "year_2024.week_5.analysis.html#groundhog-predictions-and-geographic-location",
    "href": "year_2024.week_5.analysis.html#groundhog-predictions-and-geographic-location",
    "title": "Week 5 - Groundhog Predictions",
    "section": "Groundhog predictions and geographic location",
    "text": "Groundhog predictions and geographic location\nI’m curious whether groundhogs (or other creatures) that live further north are more likely to predict more winter. It definitely feels like winter is never going to end up here most years, especially in February.\nTo explore this, we can compute the proportion of “continued winter” predictions per groundhog, and plot that against latitude.\n\n(-&gt; predictions\n    (tc/group-by [:id])\n    (tc/aggregate {:winter-percent #(double (/ (-&gt;&gt; % :shadow (remove false?) count)\n                                               (tc/row-count %)))})\n    (tc/inner-join groundhogs [:id])\n    (tc/select-columns [:id :winter-percent :latitude])\n    (hanami/plot ht/point-chart\n                 {:X :latitude\n                  :XSCALE {:zero false}\n                  :Y :winter-percent}))\n\n\n\n\nIt doesn’t really look like there’s any relationship here. We can quantify it by computing the correlation coefficient between these two values. This is a simple calculation that’s built right in to tech.ml.dataset, the library underlying tablecloth, so we can pass our columns right into it. We also have to tell it which type of correlation to compute (:pearson, :spearman, and :kendall are supported).\n\n(let [ds (-&gt; predictions\n             (tc/group-by [:id])\n             (tc/aggregate {:winter-percent #(double (/ (-&gt;&gt; % :shadow (remove false?) count)\n                                                        (tc/row-count %)))})\n             (tc/inner-join groundhogs [:id]))]\n  (tmdcol/correlation (:winter-percent ds) (:latitude ds) :pearson))\n\n\n0.0876661440031309\n\nThis is effectively no correlation, so interestingly enough groundhogs living further north do not disproportionately predict longer winters.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5 - Groundhog Predictions</span>"
    ]
  },
  {
    "objectID": "year_2024.week_5.analysis.html#winter-vs.-spring-predictions",
    "href": "year_2024.week_5.analysis.html#winter-vs.-spring-predictions",
    "title": "Week 5 - Groundhog Predictions",
    "section": "Winter vs. spring predictions",
    "text": "Winter vs. spring predictions\nThe last thing we can look at is the proportion of groundhogs that predict winter vs spring each year. We’ll select only years since 1980, when groundhog day started to take off, and filter out rows that have no prediction\n\n(-&gt; predictions\n    (tc/select-rows #(&lt; 1980 (:year %)))\n    (tc/select-rows #(not (nil? (:shadow %))))\n    (hanami/plot ht/bar-chart\n                 {:X :year\n                  :XTYPE :temporal\n                  :YAGG \"count\"\n                  :YSTACK \"normalize\"\n                  :YTITLE \"Percentage of predictions\"\n                  :YGRID false\n                  :COLOR {:field \"shadow\" :scale {:range [\"gold\" \"lightblue\"]}\n                          :legend {:title \"Prediction\"\n                                   :labelExpr \"{'true': 'Extended winter', 'false': 'Early spring'}[datum.label]\"}}\n                  :WIDTH 700\n                  :MSIZE 13\n                  :BACKGROUND \"white\"}))\n\n\n\n\nSo there we have it, some visualizations exploring the groundhog day data. On to next week. See you then :)\n\n\n\n\nsource: src/year_2024/week_5/analysis.clj",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5 - Groundhog Predictions</span>"
    ]
  },
  {
    "objectID": "year_2024.week_6.analysis.html",
    "href": "year_2024.week_6.analysis.html",
    "title": "Week 6 - A Few World Heritage Sites",
    "section": "",
    "text": "Bar charts and variations\nFirst we’ll load our data. It isn’t downloaded this time, we’ll just construct it manually. There are lots of ways to pass a “dataset literal” to tablecloth, one of the more succinct ways is as a map with keys as column names and values as column values.\nThis data isn’t “tidy”, so first we’ll clean it up to make it so. This means we’ll structure our data such that each column contains only one variable, each row contains one observation, and each cell contains a single value. In this tiny dataset our variables (things that measure a given attribute/property of each item) are “year” and “world heritage site count”. The items or units we’re measuring are the countries, and so each cell should contain a single value for a given country/year combination. Then we can make a simple stacked bar chart.\nThis works, but it doesn’t tell a super clear story right away. Stacking different sized rectangles on top of each other is not an ideal way to convey which ones are important and which aren’t. A grouped bar chart might be more intuitive. We can also add labels by adding a text layer.\nThis is slightly better, we can more easily compare the countries now, but this still doesn’t draw our attention to the most noteworthy thing about the data. Grouping the data by country instead of year tells a different story.\nThis makes it immediately obvious that Denmark saw the biggest increase in its number of world heritage sites between 2004 and 2012. Another way to organize the data is to stack the years. Adding the labels and getting the years in the right order is a little bit cumbersome, but possible:\nAll this is definitely making me reflect on what’s missing from Clojure’s dataviz story. For one, it would be challenging to create many of the other types of charts from the example website. Even making these various bar charts is a pretty cumbersome and verbose. There’s definitely space for a more succinct, grammar-based viz library that doesn’t conflate aesthetic concerns with data concerns so much. For example, we shouldn’t have to care at all about the underlying implementation of a viz library to sort two marks on the screen, but here we have to dig into the internals of vega lite to manually compute a new column in the underlying dataset that gets passed to the renderer in order to tell it how to sort them. Ideally that would be abstracted away behind a simple API.\nAnyway, obviously vega-lite is an amazing library, and it’s fun bumping up against its edges often enough to get some ideas about what would be interesting to change or improve – although also dangerous! I’m not sure I need any more projects right now 😄\nI think in the interest of time I’ll wrap up this week’s explorations here to give myself some extra bandwidth to dive more into the world of data viz and graphics libraries. Wish me luck, and see you next week :)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 6 - A Few World Heritage Sites</span>"
    ]
  },
  {
    "objectID": "year_2024.week_6.analysis.html#bar-charts-and-variations",
    "href": "year_2024.week_6.analysis.html#bar-charts-and-variations",
    "title": "Week 6 - A Few World Heritage Sites",
    "section": "",
    "text": "(def heritage-sites\n  (-&gt; {:country [\"Norway\" \"Denmark\" \"Sweden\"]\n       :2004 [5 4 13]\n       :2022 [8 10 15]}\n      tc/dataset))\n\n\n\n(def tidy-heritage-sites\n  ;; tidy up the data, pivoting it longer so we have one value per cell\n  (-&gt; heritage-sites\n      (tc/pivot-&gt;longer [:2004 :2022] {:target-columns :year\n                                       :value-column-name :count\n                                       :splitter #\":(\\d{4})\"\n                                       :datatypes {:year :int16}})))\n\n\n(-&gt; tidy-heritage-sites\n    ;; make a simple stacked bar chart\n    (hanami/plot ht/bar-chart\n                 {:X :year\n                  :XTYPE :ordinal\n                  :XAXIS {:labelAngle -45 :title \"Year\"}\n                  :Y :count\n                  :YTITLE \"World Heritage Sites\"\n                  :COLOR {:field :country :title \"Country\"}}))\n\n\n\n\n\n\n(-&gt; tidy-heritage-sites\n    (hanami/plot (assoc ht/layer-chart :encoding :ENCODING)\n                 {:X :year\n                  :XTYPE :ordinal\n                  :Y :count\n                  :LAYER [{:mark {:type \"bar\"}\n                           :encoding {:color {:field \"country\"}}}\n                          {:mark {:type \"text\" :dy 140}\n                           :encoding {:text {:field \"count\" :type \"quantitative\"}\n                                      :y {:scale {:zero false}}}}]})\n    (assoc-in [:encoding :xOffset] {:field \"country\"}))\n\n\n\n\n\n\n(-&gt; tidy-heritage-sites\n    (hanami/plot (assoc ht/layer-chart :encoding :ENCODING)\n                 {:X :country\n                  :XTYPE :ordinal\n                  :Y :count\n                  :LAYER [{:mark {:type \"bar\"}\n                           :encoding {:color {:field \"year\"}}}\n                          {:mark {:type \"text\" :dy 140}\n                           :encoding {:text {:field \"count\" :type \"quantitative\"}\n                                      :y {:scale {:zero false}}}}]})\n    (assoc-in [:encoding :xOffset] {:field \"year\"}))\n\n\n\n\n\n\n(-&gt; tidy-heritage-sites\n    (hanami/plot ht/layer-chart\n                 {:TRANSFORM [{:calculate \"indexof(['2004', '2022'], datum.year)\" :as \"order\"}]\n                  :LAYER [{:mark {:type \"bar\"}\n                           :encoding {:x {:field :country :type :ordinal}\n                                      :y {:field :count :type :quantitative}\n                                      :color {:field :year}\n                                      :order {:field :order}}}\n                          {:mark {:type \"text\" :dy 14}\n                           :encoding {:text {:field :year :type :quantitative}\n                                      :x {:field :country :type \"nominal\"}\n                                      :y {:field :count :type :quantitative :stack \"zero\"}\n                                      :order {:field :order}}}]}))\n\n\n\n\n\n\n\n\n\n\n\nsource: src/year_2024/week_6/analysis.clj",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 6 - A Few World Heritage Sites</span>"
    ]
  },
  {
    "objectID": "year_2024.week_7.analysis.html",
    "href": "year_2024.week_7.analysis.html",
    "title": "Week 7 - Valentine’s Day Consumer Data",
    "section": "",
    "text": "Historical spending\nThe first and easiest thing to plot is just spending over time. This isn’t linear data (i.e. there’s no inferred value missing between the years), so we’ll just make a bar chart.\nWe can see spending is increasing over time. It might also be interesting to see how the proportion of people celebrating Valentine’s day is changing over time:\nFrom this we can clearly see it’s decreasing. If we change the scale of the y axis though it makes the decrease seem less pronounced. We can also format the y axis values – to do this we’ll have to update the column to support the new formatting.\nWe could compute the percentage change in spending year over year per category, to see how the spending habits by category have changed over the period we have data for. First we’ll get the first and last years that we have data for, then we’ll compute the percentage change for each category.\nWe can see there are 13 rows, so we’ll just make extra sure they’re sorted by year and then manually pick out the first and last ones. Then we’ll want to pivot the data to lengthen it so we have one column for the categories and one for the amounts.\nThis is where I ran into the first problem with the data. Sorting by year should work here, but instead we get an error about not being able to find the “Year” column.\nThe “Year” column header looks right:\nBut if we inspect the first character of it, we can see it’s misleading:\n(That should just be “Y”)\nWe can find out what actual character this is by dropping into Java:\nSo this first character is actually the unicode character “U+FEFF”, the dreaded “zero-width no-break space”, also known as the byte order mark. For historical and other reasons, sometimes files start with this nefarious character.\nThere are lots of ways to fix this, one is to just strip the character from the CSV first and then create our dataset. We’ll have to create it a little more manually since we’ll be passing the data directly the dataset constructor instead of reading it from a file:\nNow we should be able to pivot our data to re-organize it and make our grouped bar chart:\n:_unnamed [14 5]:\nGreat. Now we can get back to computing the percent changes between the years. There are lots of ways to do this. I think the easiest is to pivot our data to tidy it up, then fold the rows by Category to group the pairs of values to work with.\n_unnamed [7 6]:\nNow we can plot the percentage increase by category. I’ll sort by the increase to make the chart look nicer.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7 - Valentine's Day Consumer Data</span>"
    ]
  },
  {
    "objectID": "year_2024.week_7.analysis.html#historical-spending",
    "href": "year_2024.week_7.analysis.html#historical-spending",
    "title": "Week 7 - Valentine’s Day Consumer Data",
    "section": "",
    "text": "(-&gt; historical-spending\n    (hanami/plot ht/bar-chart\n                 {:X \"Year\"\n                  :XTYPE \"ordinal\"\n                  :XAXIS {:labelAngle -45 :title \"Year\"}\n                  :Y \"PerPerson\"\n                  :YAXIS {:format \"$.0f\" :title \"Spending per person\"}}))\n\n\n\n\n\n\n(-&gt; historical-spending\n    (hanami/plot ht/bar-chart\n                 {:X \"Year\"\n                  :XTYPE \"ordinal\"\n                  :XAXIS {:labelAngle -45 :title \"Year\"}\n                  :Y \"PercentCelebrating\"}))\n\n\n\n\n\n\n(-&gt; historical-spending\n    (tc/update-columns {\"PercentCelebrating\"\n                        (partial map #(-&gt; % (/ 100) double))})\n    (hanami/plot ht/bar-chart\n                 {:X \"Year\"\n                  :XTYPE \"ordinal\"\n                  :XAXIS {:labelAngle -45 :title \"Year\"}\n                  :Y \"PercentCelebrating\"\n                  :YSCALE {:domain [0 1]}\n                  :YAXIS {:format \"1%\" :title \"Percent celebrating\"}}))\n\n\n\n\n\n\n\n\n(tc/row-count historical-spending)\n\n\n13\n\n(-&gt; historical-spending\n(tc/order-by \"Year\")\n(tc/select-rows [0 12]))\n\n\n(-&gt; historical-spending\n    tc/column-names\n    first)\n\n\n\"﻿Year\"\n\n\n\n(-&gt; historical-spending\n    tc/column-names\n    first\n    first)\n\n\n\\﻿\n\n\n\n\n(-&gt; historical-spending\n    tc/column-names\n    first\n    first\n    ;; first convert the character to its integer value\n    int\n    (Integer/toHexString))\n\n\n\"feff\"\n\n\n\n\n(def historical-spending\n  (let [data (-&gt; \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-13/historical_spending.csv\"\n                 slurp\n                 (subs 1)\n                 charred/read-csv)]\n    (tc/dataset (rest data) {:column-names (first data)})))\n\n\n\n(-&gt; historical-spending\n    (tc/order-by \"Year\")\n    (tc/select-rows [0 12])\n    (tc/pivot-&gt;longer (complement #{\"Year\" \"PercentCelebrating\" \"PerPerson\"})\n                      {:target-columns \"Category\"\n                       :value-column-name \"Amount\"}))\n\n\n\n\n\nYear\nPercentCelebrating\nPerPerson\nCategory\nAmount\n\n\n\n\n2010\n60\n103.00\nCandy\n8.60\n\n\n2022\n53\n175.41\nCandy\n15.90\n\n\n2010\n60\n103.00\nFlowers\n12.33\n\n\n2022\n53\n175.41\nFlowers\n16.71\n\n\n2010\n60\n103.00\nJewelry\n21.52\n\n\n2022\n53\n175.41\nJewelry\n45.75\n\n\n2010\n60\n103.00\nGreetingCards\n5.91\n\n\n2022\n53\n175.41\nGreetingCards\n7.47\n\n\n2010\n60\n103.00\nEveningOut\n23.76\n\n\n2022\n53\n175.41\nEveningOut\n31.35\n\n\n2010\n60\n103.00\nClothing\n10.93\n\n\n2022\n53\n175.41\nClothing\n21.46\n\n\n2010\n60\n103.00\nGiftCards\n8.42\n\n\n2022\n53\n175.41\nGiftCards\n17.22\n\n\n\n\n\n(-&gt; historical-spending\n    (tc/order-by \"Year\")\n    (tc/select-rows [12 0])\n    (tc/pivot-&gt;longer (complement #{\"Year\" \"PercentCelebrating\" \"PerPerson\"})\n                      {:target-columns \"Category\"\n                       :value-column-name \"Amount\"})\n    (tc/convert-types [\"Amount\"] :float64)\n    (tc/fold-by \"Category\")\n    (tc/map-columns \"Increase\" [\"Amount\"] (fn [[new old]]\n                                            (-&gt; new (- old) (/ old)))))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategory\nYear\nPercentCelebrating\nPerPerson\nAmount\nIncrease\n\n\n\n\nCandy\n[2022 2010]\n[53 60]\n[175.41 103.00]\n[15.9 8.6]\n0.84883721\n\n\nFlowers\n[2022 2010]\n[53 60]\n[175.41 103.00]\n[16.71 12.33]\n0.35523114\n\n\nJewelry\n[2022 2010]\n[53 60]\n[175.41 103.00]\n[45.75 21.52]\n1.12592937\n\n\nGreetingCards\n[2022 2010]\n[53 60]\n[175.41 103.00]\n[7.47 5.91]\n0.26395939\n\n\nEveningOut\n[2022 2010]\n[53 60]\n[175.41 103.00]\n[31.35 23.76]\n0.31944444\n\n\nClothing\n[2022 2010]\n[53 60]\n[175.41 103.00]\n[21.46 10.93]\n0.96340348\n\n\nGiftCards\n[2022 2010]\n[53 60]\n[175.41 103.00]\n[17.22 8.42]\n1.04513064\n\n\n\n\n\n(-&gt; historical-spending\n    (tc/order-by \"Year\")\n    (tc/select-rows [12 0])\n    (tc/pivot-&gt;longer (complement #{\"Year\" \"PercentCelebrating\" \"PerPerson\"})\n                      {:target-columns \"Category\"\n                       :value-column-name \"Amount\"})\n    (tc/convert-types [\"Amount\"] :float64)\n    (tc/fold-by \"Category\")\n    (tc/map-columns \"Increase\" [\"Amount\"] (fn [[new old]]\n                                            (-&gt; new (- old) (/ old))))\n    (hanami/plot ht/bar-chart\n                 {:X \"Category\"\n                  :XTYPE :nominal\n                  :XSORT \"y\"\n                  :Y \"Increase\"}))",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7 - Valentine's Day Consumer Data</span>"
    ]
  },
  {
    "objectID": "year_2024.week_7.analysis.html#spending-differences-between-men-and-women",
    "href": "year_2024.week_7.analysis.html#spending-differences-between-men-and-women",
    "title": "Week 7 - Valentine’s Day Consumer Data",
    "section": "Spending differences between men and women",
    "text": "Spending differences between men and women\nWe also have data about spending differences between men and women. We can compare those side by side if we rearrange the data a little bit first. Our gifts-by-gender dataset has a separate column per category. We’ll want to rearrange the data so that the category is a single variable with each value in its own row.\nThis dataset has the same issue as the last one, so we’ll re-fetch it and strip the first character.\n\n(def gifts-gender\n  (let [data (-&gt; \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-13/gifts_gender.csv\"\n                 slurp\n                 (subs 1)\n                 charred/read-csv)]\n    (tc/dataset (rest data) {:column-names (first data)})))\n\nNow we should be able to pivot our data to re-organize it and make our grouped bar chart:\n\n(-&gt; gifts-gender\n    (tc/pivot-&gt;longer (complement #{\"Gender\" \"SpendingCelebrating\"})\n                      {:target-columns \"Category\"\n                       :value-column-name \"Percentage\"})\n    (hanami/plot ht/bar-chart\n                 {:X \"Category\"\n                  :XTYPE :nominal\n                  :Y \"Percentage\"\n                  :COLOR \"Gender\"})\n    (assoc-in [:encoding :xOffset] {:field \"Gender\"}))\n\n\n\n\nHere we can see pretty clearly that men are significantly more likely to spend on flowers and jewelry than women, but the other categories are more evenly matched.\nWe also have data about spending by age, so we can explore a bit how spending habits change with age. We’ll start again with fixing the dataset and pivoting the data to make a column for categories and a column for the percentages.\n\n(def gifts-age\n  (let [data (-&gt; \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-13/gifts_age.csv\"\n                 slurp\n                 (subs 1)\n                 charred/read-csv)]\n    (tc/dataset (rest data) {:column-names (first data)})))\n\nI think the most informative way to plot this would be to see the categories grouped together, comparing the age ranges side by side within a category.. if that makes any sense. Anyway like this:\n\n(-&gt; gifts-age\n    (tc/pivot-&gt;longer (complement #{\"Age\" \"SpendingCelebrating\"})\n                      {:target-columns \"Category\"\n                       :value-column-name \"Percentage\"})\n    (hanami/plot ht/bar-chart\n                 {:X \"Category\"\n                  :XTYPE :nominal\n                  :Y \"Percentage\"\n                  :COLOR \"Age\"})\n    (assoc-in [:encoding :xOffset] {:field \"Age\"}))\n\n\n\n\nThis shows a pretty clear trend that spending in all categories except for greeting cards seems to decrease with age. This is interesting, but seeing it makes me wonder what the main message would be if we reversed the age/category encodings:\n\n(-&gt; gifts-age\n    (tc/pivot-&gt;longer (complement #{\"Age\" \"SpendingCelebrating\"})\n                      {:target-columns \"Category\"\n                       :value-column-name \"Percentage\"})\n    (hanami/plot ht/bar-chart\n                 {:X \"Age\"\n                  :XTYPE :nominal\n                  :Y \"Percentage\"\n                  :COLOR \"Category\"})\n    (assoc-in [:encoding :xOffset] {:field \"Category\"}))\n\n\n\n\nI still think the first one is better, but I suppose it depends what information you’re looking for. From this one we can see that people of all ages are likely to spend on candy and greeting cards, and that gift cards are the least common gift no matter the age.\nWell.. there are tons of questions we could ask of this data, but I’ll leave it here for this week. See you next week :)\n\n\n\n\nsource: src/year_2024/week_7/analysis.clj",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7 - Valentine's Day Consumer Data</span>"
    ]
  }
]